filter(Treatment %in% check_treatments) %>%
mutate(Treatment = replace(Treatment, Treatment %in% CT & aggregate_ctls, "CT"), Treatment)  %>% #Choose index to graph
rename_at(vars(contains(which_index)), list( ~ sub(which_index, 'index', .)))
index_subset
ggplot(data = index_subset, mapping = aes(x = Date, y = index, color = Treatment)) +
geom_point(aes(shape=Block), position = "jitter") +
geom_smooth(aes(fill=Treatment), method = lm, formula = y ~ splines::bs(x, 3)) +
facet_wrap(vars(Site)) +
scale_color_manual(values=c("CT" = "forestgreen", "CT1"="darkolivegreen2", "CT2"="darkolivegreen3",
"N" = "dodgerblue", "NO3" = "skyblue", "NH4" = "deepskyblue",
"P" = "red2",
"NP" = np_colors[5],
"F0.5" = np_colors[1],
"F1" = np_colors[2],
"F2" = np_colors[3],
"F5" = np_colors[4],
"F10" = np_colors[5])) +
scale_fill_manual(values=c("CT" = "forestgreen", "CT1"="darkolivegreen2", "CT2"="darkolivegreen3",
"N" = "dodgerblue", "NO3" = "skyblue", "NH4" = "deepskyblue",
"P" = "red2",
"NP" = np_colors[5],
"F0.5" = np_colors[1],
"F1" = np_colors[2],
"F2" = np_colors[3],
"F5" = np_colors[4],
"F10" = np_colors[5]))
ggplot(data = index_subset,aes(x=Date, y=NDVI)) +
geom_point(aes(color=Treatment, shape = Block))
ggplot(data = index_subset,aes(x=Date, y=index)) +
geom_point(aes(color=Treatment, shape = Block))
ggplot(data = index_subset, aes(x=FileNum,y=index, fill=Block)) +
geom_bar(stat="identity") + facet_wrap(vars(Block), scales = "free_x")
ggplot(data = index_subset, aes(x=Date,y=Value)) +
geom_point(aes(color=Block, shape=factor(Replicate)))
spec_subset <- df_subset %>% select(-Spectra) %>%
filter(Date == check_date) %>%
left_join(spu_dataframe)
ggplot(data = index_subset, aes(x=Date,y=Value)) +
geom_point(aes(color=Block, shape=factor(Replicate)))
knitr::opts_chunk$set(echo = TRUE)
## Required Packages
library("tidyverse")
library("knitr")
source("UnispecProtocol/unispec_protocol_functions.R") # file loads required functions
## Data Path
data_path <-  "/Users/toolik/OneDrive - Marine Biological Laboratory/Toolik Terrestrial/UnispecData/2019/"
## for Ruby's chromebook
data_path <- "UnispecRecord/UnispecData/2019/"
dir_year <- "2019"
Site_Names <- list(DHT = "DHT", DH ="DHT", LDHT = "DHT", DHTB = "DHT", HTH = "DHT", DHTPC = "DHT", HST = "HIST", HIS="HIST",
LOF = "LMAT",  LOFB = "LMAT", LNB = "LMAT", LOFRB ="LMAT",
MATB="MAT", MATSL= "MAT", MATBK = "MAT",
MANTB ="MNAT",MNATB ="MNAT", NAMTB = "MNAT",
NMNT = "NANT", NANTB ="NANT", JULNB ="NANT",NMNTB ="NANT",
LSHB= "SHB", SHBB = "SHB", SHRBB = "SHB", SHRB = "SHB",
LWSG = "WSG", WSGB = "WSG", WS ="WSG", WSB = "WSG", WSDB = "WSG") #TEST = "LAB2"
band_defns <- tribble(
~definition, ~color, ~min, ~max,
"ITEX", "red", 560, 600,
"ITEX", "nir", 725, 1000,
"MODIS", "red", 620, 670,
"MODIS", "nir", 841, 876,
"MODIS", "blue", 459,479,
"SKYE", "red", 620, 680,
"SKYE", "nir", 830, 880,
"SKYE", "blue", 455, 480,
"ToolikGIS_Drone_2018", "red", 640, 680,
"ToolikGIS_Drone_2018", "nir", 820, 890,
"ToolikGIS_MicaSense_2019", "blue", 455, 495,
"ToolikGIS_MicaSense_2019", "green", 540, 580,
"ToolikGIS_MicaSense_2019", "red", 658, 678,
"ToolikGIS_MicaSense_2019", "red_edge", 707, 727,
"ToolikGIS_MicaSense_2019", "near_ir", 800, 880,
"ToolikEDC", "red", 560, 680,
"ToolikEDC", "nir", 725, 1000
)
## ## Processed Data
processed_filename <- paste0(data_path, dir_year, "_processed_spu_data.rds")
processed_spectra <- read_rds(processed_filename)
field_key <- paste0(data_path, "2019_unispec_field_key.csv")
unispec_field_key <- read_csv(field_key, skip_empty_rows = T, col_types = cols(
Date = col_character(),
Site = col_character(),
Block = col_character(),
Treatment = col_character(),
Replicate = col_double(),
Location = col_character(),
FileNum = col_double(),
Weather = col_character(),
Notes = col_character()
)) ## remove NA rows
unique(spu_dataframe$Site) %>% sort()
processed_spectra
unispec_key
unispec_field_key
unispec_field_key %>% filter(!is.na(X10))
unispec_field_key %>% filter(!is.na(X11))
unispec_field_key %>% filter(!is.na(X12))
?read_csv
unispec_field_key %>% cols()
unispec_field_key <- read_csv(field_key, skip_empty_rows = T, col_types = cols(
Date = col_character(),
Site = col_character(),
Block = col_character(),
Treatment = col_character(),
Replicate = col_double(),
Location = col_character(),
FileNum = col_double(),
Weather = col_character(),
Notes = col_character()
)) ## remove NA rows
unispec_field_key <- read_csv(field_key, skip_empty_rows = T, col_only = cols(
Date = col_character(),
Site = col_character(),
Block = col_character(),
Treatment = col_character(),
Replicate = col_double(),
Location = col_character(),
FileNum = col_double(),
Weather = col_character(),
Notes = col_character()
)) ## remove NA rows
unispec_field_key <- read_csv(field_key, skip_empty_rows = T, cols_only = cols(
Date = col_character(),
Site = col_character(),
Block = col_character(),
Treatment = col_character(),
Replicate = col_double(),
Location = col_character(),
FileNum = col_double(),
Weather = col_character(),
Notes = col_character()
)) ## remove NA rows
unispec_field_key <- read_csv(field_key, skip_empty_rows = T) ## remove NA rows
field_key <- paste0(data_path, "2019_unispec_field_key.csv")
unispec_field_key <- read_csv(field_key, skip_empty_rows = T) ## remove NA rows
## for Ruby's chromebook
data_path <- "UnispecRecord/UnispecData/2019_unispec_data"
band_defns <- tribble(
~definition, ~color, ~min, ~max,
"ITEX", "red", 560, 600,
"ITEX", "nir", 725, 1000,
"MODIS", "red", 620, 670,
"MODIS", "nir", 841, 876,
"MODIS", "blue", 459,479,
"SKYE", "red", 620, 680,
"SKYE", "nir", 830, 880,
"SKYE", "blue", 455, 480,
"ToolikGIS_Drone_2018", "red", 640, 680,
"ToolikGIS_Drone_2018", "nir", 820, 890,
"ToolikGIS_MicaSense_2019", "blue", 455, 495,
"ToolikGIS_MicaSense_2019", "green", 540, 580,
"ToolikGIS_MicaSense_2019", "red", 658, 678,
"ToolikGIS_MicaSense_2019", "red_edge", 707, 727,
"ToolikGIS_MicaSense_2019", "near_ir", 800, 880,
"ToolikEDC", "red", 560, 680,
"ToolikEDC", "nir", 725, 1000
)
## Read .spu raw files
raw_data_path <- paste0(data_path, "raw_spu_files")
raw_data_path
## for Ruby's chromebook
data_path <- "UnispecRecord/UnispecData/2019_unispec_data/"
## Read .spu raw files
raw_data_path <- paste0(data_path, "raw_spu_files")
spu_files <- list.files(path = raw_data_path, pattern = ".spu$", full.names = T, recursive=T)
new_spu_files <- str_subset(spu_files, file_match, negate = T)
knitr::opts_chunk$set(echo = TRUE)
## Required Packages
library("tidyverse")
library("knitr")
source("UnispecProtocol/unispec_protocol_functions.R") # file loads required functions
## Data Path
data_path <-  "/Users/toolik/OneDrive - Marine Biological Laboratory/Toolik Terrestrial/UnispecData/2019/"
## for Ruby's chromebook
data_path <- "UnispecRecord/UnispecData/2019_unispec_data/"
band_defns <- tribble(
~definition, ~color, ~min, ~max,
"ITEX", "red", 560, 600,
"ITEX", "nir", 725, 1000,
"MODIS", "red", 620, 670,
"MODIS", "nir", 841, 876,
"MODIS", "blue", 459,479,
"SKYE", "red", 620, 680,
"SKYE", "nir", 830, 880,
"SKYE", "blue", 455, 480,
"ToolikGIS_Drone_2018", "red", 640, 680,
"ToolikGIS_Drone_2018", "nir", 820, 890,
"ToolikGIS_MicaSense_2019", "blue", 455, 495,
"ToolikGIS_MicaSense_2019", "green", 540, 580,
"ToolikGIS_MicaSense_2019", "red", 658, 678,
"ToolikGIS_MicaSense_2019", "red_edge", 707, 727,
"ToolikGIS_MicaSense_2019", "near_ir", 800, 880,
"ToolikEDC", "red", 560, 680,
"ToolikEDC", "nir", 725, 1000
)
spu_files <- list.files(path = raw_data_path, pattern = ".spu$", full.names = T, recursive=T)
new_spu_files <- str_subset(spu_files, file_match, negate = T)
# Read all the metadata from the spu files using function read_spufile_metadata; add a variable with the filename
#  and max of ChA and ChB.
spu_metadata <- map_dfr(new_spu_files, read_spu_file_metadata) %>%
mutate(spu_filename_full = new_spu_files)
## Read .spu raw files
raw_data_path <- paste0(data_path, "raw_spu_files")
spu_files <- list.files(path = raw_data_path, pattern = ".spu$", full.names = T, recursive=T)
# Read all the metadata from the spu files using function read_spufile_metadata; add a variable with the filename
#  and max of ChA and ChB.
spu_metadata <- map_dfr(new_spu_files, read_spu_file_metadata) %>%
mutate(spu_filename_full = new_spu_files)
# Read all the metadata from the spu files using function read_spufile_metadata; add a variable with the filename
#  and max of ChA and ChB.
spu_metadata <- map_dfr(spu_files, read_spu_file_metadata) %>%
mutate(spu_filename_full = new_spu_files)
# Read all the metadata from the spu files using function read_spufile_metadata; add a variable with the filename
#  and max of ChA and ChB.
spu_metadata <- map_dfr(spu_files, read_spu_file_metadata) %>%
mutate(spu_filename_full = spu_files)
spu_data <- spu_metadata %>%
mutate(Spectra=map(spu_filename_full, function(x) read_spu_file_spectra(x)))
# > Set Type of Scan -----------------------------------------------------
spu_dataframe <- spu_dataframe %>%
mutate(Type = ifelse(grepl("DARKscan",Remarks, fixed=T), "Darkscan",
ifelse(grepl("Datascan,DC",Remarks, fixed=T), "Throwawayscan", NA))) %>%
distinct(DateTime, spu_filename, .keep_all = T) %>%
mutate(Date = as.character(Date))
## Data summary
spu_dataframe %>%   group_by(Date, Site) %>%
summarize(Files = n_distinct(spu_filename)) %>%
kable()
# > Set Type of Scan -----------------------------------------------------
spu_dataframe <- spu_data %>%
mutate(Type = ifelse(grepl("DARKscan",Remarks, fixed=T), "Darkscan",
ifelse(grepl("Datascan,DC",Remarks, fixed=T), "Throwawayscan", NA))) %>%
distinct(DateTime, spu_filename, .keep_all = T) %>%
mutate(Date = as.character(Date))
# Read all the metadata from the spu files using function read_spufile_metadata; add a variable with the filename
#  and max of ChA and ChB.
spu_metadata <- map_dfr(spu_files, read_spu_file_metadata) %>%
mutate(spu_filename_full = spu_files)
spu_data <- spu_metadata %>%
mutate(Spectra=map(spu_filename_full, function(x) read_spu_file_spectra(x)))
## Read .spu raw files
raw_data_path <- paste0(data_path, "raw_spu_files")
spu_files <- list.files(path = raw_data_path, pattern = ".spu$", full.names = T, recursive=T)
raw_data_path
data_path
## for Ruby's chromebook
data_path <- paste0("UnispecRecord/UnispecData/2019_unispec_data/", date)
## Session Date
date <- "2019-08-16"
## for Ruby's chromebook
data_path <- paste0("UnispecRecord/UnispecData/2019_unispec_data/", date)
## for Ruby's chromebook
data_path <- paste0("UnispecRecord/UnispecData/2019_unispec_data/", session_date)
## Session Date
session_date <- "2019-08-16"
## for Ruby's chromebook
data_path <- paste0("UnispecRecord/UnispecData/2019_unispec_data/", session_date)
data_path
## Read .spu raw files
raw_data_path <- paste0(data_path, "raw_spu_files")
spu_files <- list.files(path = raw_data_path, pattern = ".spu$", full.names = T, recursive=T)
raw_data_path
## for Ruby's chromebook
data_path <- paste0("UnispecRecord/UnispecData/2019_unispec_data/", session_date, "/")
## Read .spu raw files
spu_files <- list.files(path = data_path, pattern = ".spu$", full.names = T, recursive=T)
data_path
# Read all the metadata from the spu files using function read_spufile_metadata; add a variable with the filename
#  and max of ChA and ChB.
spu_metadata <- map_dfr(spu_files, read_spu_file_metadata) %>%
mutate(spu_filename_full = spu_files)
spu_data <- spu_metadata %>%
mutate(Spectra=map(spu_filename_full, function(x) read_spu_file_spectra(x)))
# > Set Type of Scan -----------------------------------------------------
spu_dataframe <- spu_data %>%
mutate(Type = ifelse(grepl("DARKscan",Remarks, fixed=T), "Darkscan",
ifelse(grepl("Datascan,DC",Remarks, fixed=T), "Throwawayscan", NA))) %>%
distinct(DateTime, spu_filename, .keep_all = T) %>%
mutate(Date = as.character(Date))
## Data summary
spu_dataframe %>%   group_by(Date, Site) %>%
summarize(Files = n_distinct(spu_filename)) %>%
kable()
spu_files
spu_metadata
spu_data
spu_file <- spu_files[1]
spu_file
# Filename metadata
filename_metadata <- unlist(str_split(filename, pattern = "/")) %>% last() %>% str_split("_") %>% unlist()
filename <- spu_files[1]
# Filename metadata
filename_metadata <- unlist(str_split(filename, pattern = "/")) %>% last() %>% str_split("_") %>% unlist()
Site <- filename_metadata[2]
Date <- filename_metadata[1]
FileNum <- as.integer(str_extract(filename_metadata[3], "\\d{5}")) # extract 5 digits
Site
Date
FileNum
# Extract info from the file itself, reading metadata from first 9 lines. Create a dataframe
text <- read_lines(filename, n_max=9)
text
DateTime <-  lubridate::mdy_hms(text[3], tz="America/Anchorage")
Date <- lubridate::date(DateTime)
Integration_ms <- as.numeric(strsplit(text[8], split = " ")[[1]][3])
Temp <- as.numeric(strsplit(strsplit(text[5], split = " ")[[1]][4], split="=")[[1]][2])
Remarks <- text[2]
# Truncated Filename
Spufilename <- unlist(str_split(filename, pattern = "/")) %>% last()
# Metadata
metaData <- tibble(FileNum=FileNum, Date=Date, DateTime=DateTime, Integration_ms=Integration_ms,
Temp=Temp, Remarks=Remarks, spu_filename=Spufilename )
metaData
FileNum <- as.integer(str_extract(filename, "\\d{5}")) # extract 5 digits
FileNum
FileNum <- as.integer(str_extract(filename, "\\d{5}")) # extract 5 digits
read_spu_file_metadata <- function(filename) {
ruby_year <- str_detect(filename, "2017|2018|2019")
if (ruby_year) {
## Reads a .spu file from 2017 or 2018
# Filename metadata
# filename_metadata <- unlist(str_split(filename, pattern = "/")) %>% last() %>% str_split("_") %>% unlist()
# Site <- filename_metadata[2]
# Date <- filename_metadata[1]
FileNum <- as.integer(str_extract(filename, "\\d{5}")) # extract 5 digits
# Extract info from the file itself, reading metadata from first 9 lines. Create a dataframe
text <- read_lines(filename, n_max=9)
DateTime <-  lubridate::mdy_hms(text[3], tz="America/Anchorage")
Date <- lubridate::date(DateTime)
Integration_ms <- as.numeric(strsplit(text[8], split = " ")[[1]][3])
Temp <- as.numeric(strsplit(strsplit(text[5], split = " ")[[1]][4], split="=")[[1]][2])
Remarks <- text[2]
# Truncated Filename
Spufilename <- unlist(str_split(filename, pattern = "/")) %>% last()
# Metadata
metaData <- tibble(FileNum=FileNum, Date=Date, DateTime=DateTime, Integration_ms=Integration_ms,
Temp=Temp, Remarks=Remarks, spu_filename=Spufilename )
} else {
## Reads a generic .spu file. Written for Historic (pre-2017) years.
# Extract metadata from filenames that have format "DATE/SITE_FILENUM.spu", e.g. "2018-06-22/DHT_00000.spu"
Site <- toupper(str_replace(filename,".*[/](.*)[_]+.*$","\\1")) # get string after / & before _
# Extract metadata from filenames that have format "DATESITEFILENUM.spu", e.g. "JUN8LOF100036.spu"
if (str_length(Site) > 5) {
Site <- toupper(str_replace(filename,"(^.*?\\d{1,2})\\s*([a-zA-Z]*)(\\d{5,7}\\.spu$)","\\2"))
# For 2012 and 2013 the spu filenames have ddmmmsite format; need to remove the 3 letter month.
if (str_length(Site)> 5){
pattern <- c("MAY","JUN","JUL", "AUG")
for (i in 1:4){Site<- sub(pattern[i], "", Site)}
}
}
# Avoid issues of digits in site/block name reading in as part of FileNum
## Based on the Unispec DC using 5 digits for automatic file numbering
FileNum <- as.integer(str_replace(filename,"(^.*?\\d{1,2})(\\D*)(\\d{5})(\\.spu$)","\\3"))
# Extract info from the file itself, reading metadata from first 9 lines. Create a dataframe
text <- read_lines(filename, n_max=9)
DateTime <-  lubridate::mdy_hms(text[3], tz="America/Anchorage")
Date <- lubridate::date(DateTime)
Integration_ms <- as.numeric(strsplit(text[8], split = " ")[[1]][3])
Temp <- as.numeric(strsplit(strsplit(text[5], split = " ")[[1]][4], split="=")[[1]][2])
Remarks <- text[2]
#Extract the file name in the spu file as a check. Some file names have spaces
Spufilename <- tolower(str_replace(text[1],".*[\\\\]([A-z0-9.]\\s*)","\\1"))
Spufilename <- str_replace(Spufilename,"\"","") # remove trailing quote
#Extract
Spufilename_file <- tolower(str_replace(filename,".*[\\\\/]([A-z0-9.]\\s*)","\\1"))
metaData <- tibble(Site=Site, FileNum=FileNum, Date=Date, DateTime=DateTime, Integration_ms=Integration_ms,
Temp=Temp, Remarks=Remarks, spu_filename=Spufilename )
}
print(Spufilename) # use for error checking
return(metaData)
}
## Read .spu raw files
spu_files <- list.files(path = data_path, pattern = ".spu$", full.names = T, recursive=T)
# Read all the metadata from the spu files using function read_spufile_metadata; add a variable with the filename
#  and max of ChA and ChB.
spu_metadata <- map_dfr(spu_files, read_spu_file_metadata) %>%
mutate(spu_filename_full = spu_files)
spu_data <- spu_metadata %>%
mutate(Spectra=map(spu_filename_full, function(x) read_spu_file_spectra(x)))
# > Set Type of Scan -----------------------------------------------------
spu_dataframe <- spu_data %>%
mutate(Type = ifelse(grepl("DARKscan",Remarks, fixed=T), "Darkscan",
ifelse(grepl("Datascan,DC",Remarks, fixed=T), "Throwawayscan", NA))) %>%
distinct(DateTime, spu_filename, .keep_all = T) %>%
mutate(Date = as.character(Date))
## Data summary
spu_dataframe %>%   group_by(Date, Site) %>%
summarize(Files = n_distinct(spu_filename)) %>%
kable()
spu_dataframe
read_spu_file_metadata <- function(filename) {
ruby_year <- str_detect(filename, "2017|2018|2019")
if (ruby_year) {
## Reads a .spu file from 2017 or 2018
# Filename metadata
# filename_metadata <- unlist(str_split(filename, pattern = "/")) %>% last() %>% str_split("_") %>% unlist()
# Site <- filename_metadata[2]
# Date <- filename_metadata[1]
#FileNum <- as.integer(str_extract(filename, "\\d{5}")) # extract 5 digits
# Extract info from the file itself, reading metadata from first 9 lines. Create a dataframe
text <- read_lines(filename, n_max=9)
DateTime <-  lubridate::mdy_hms(text[3], tz="America/Anchorage")
Date <- lubridate::date(DateTime)
Integration_ms <- as.numeric(strsplit(text[8], split = " ")[[1]][3])
Temp <- as.numeric(strsplit(strsplit(text[5], split = " ")[[1]][4], split="=")[[1]][2])
Remarks <- text[2]
# Truncated Filename
Spufilename <- unlist(str_split(filename, pattern = "/")) %>% last()
# Metadata
metaData <- tibble(FileNum=FileNum, Date=Date, DateTime=DateTime, Integration_ms=Integration_ms,
Temp=Temp, Remarks=Remarks, spu_filename=Spufilename )
} else {
## Reads a generic .spu file. Written for Historic (pre-2017) years.
# Extract metadata from filenames that have format "DATE/SITE_FILENUM.spu", e.g. "2018-06-22/DHT_00000.spu"
Site <- toupper(str_replace(filename,".*[/](.*)[_]+.*$","\\1")) # get string after / & before _
# Extract metadata from filenames that have format "DATESITEFILENUM.spu", e.g. "JUN8LOF100036.spu"
if (str_length(Site) > 5) {
Site <- toupper(str_replace(filename,"(^.*?\\d{1,2})\\s*([a-zA-Z]*)(\\d{5,7}\\.spu$)","\\2"))
# For 2012 and 2013 the spu filenames have ddmmmsite format; need to remove the 3 letter month.
if (str_length(Site)> 5){
pattern <- c("MAY","JUN","JUL", "AUG")
for (i in 1:4){Site<- sub(pattern[i], "", Site)}
}
}
# Avoid issues of digits in site/block name reading in as part of FileNum
## Based on the Unispec DC using 5 digits for automatic file numbering
FileNum <- as.integer(str_replace(filename,"(^.*?\\d{1,2})(\\D*)(\\d{5})(\\.spu$)","\\3"))
# Extract info from the file itself, reading metadata from first 9 lines. Create a dataframe
text <- read_lines(filename, n_max=9)
DateTime <-  lubridate::mdy_hms(text[3], tz="America/Anchorage")
Date <- lubridate::date(DateTime)
Integration_ms <- as.numeric(strsplit(text[8], split = " ")[[1]][3])
Temp <- as.numeric(strsplit(strsplit(text[5], split = " ")[[1]][4], split="=")[[1]][2])
Remarks <- text[2]
#Extract the file name in the spu file as a check. Some file names have spaces
Spufilename <- tolower(str_replace(text[1],".*[\\\\]([A-z0-9.]\\s*)","\\1"))
Spufilename <- str_replace(Spufilename,"\"","") # remove trailing quote
#Extract
Spufilename_file <- tolower(str_replace(filename,".*[\\\\/]([A-z0-9.]\\s*)","\\1"))
metaData <- tibble(Site=Site, FileNum=FileNum, Date=Date, DateTime=DateTime, Integration_ms=Integration_ms,
Temp=Temp, Remarks=Remarks, spu_filename=Spufilename )
}
print(Spufilename) # use for error checking
return(metaData)
}
source('~/UnispecR/UnispecProtocol/unispec_protocol_functions.R')
# Read all the metadata from the spu files using function read_spufile_metadata; add a variable with the filename
#  and max of ChA and ChB.
spu_metadata <- map_dfr(spu_files, read_spu_file_metadata) %>%
mutate(spu_filename_full = spu_files)
spu_metadata
?str_extract
# Read all the metadata from the spu files using function read_spufile_metadata;
#   add a variable with the full path filename
#   get Site & FileNum columns from filename
spu_metadata <- map_dfr(spu_files, read_spu_file_metadata) %>%
mutate(spu_filename_full = spu_files) %>%
mutate(Site = str_extract(spu_filename, "[A-Z]{3}")) %>%
mutate(FileNum = str_extract(spu_filename, "\\d{5}")) # extract 5 digits
spu_metadata
spu_metadata %>% select(spu_filename, Site, FileNum)
## REQUIRED PACKAGES
library(deSolve)
?summarize
load(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
## Required Packages
library("tidyverse")
library("knitr")
source("UnispecProtocol/unispec_protocol_functions.R") # file loads required functions
## Data Path
data_path <-  "/Users/toolik/OneDrive - Marine Biological Laboratory/Toolik Terrestrial/UnispecData/2019/"
## for Ruby's chromebook
data_path <- "UnispecRecord/UnispecData/2019/"
dir_year <- "2019"
Site_Names <- list(DHT = "DHT", DH ="DHT", LDHT = "DHT", DHTB = "DHT", HTH = "DHT", DHTPC = "DHT", HST = "HIST", HIS="HIST",
LOF = "LMAT",  LOFB = "LMAT", LNB = "LMAT", LOFRB ="LMAT",
MATB="MAT", MATSL= "MAT", MATBK = "MAT",
MANTB ="MNAT",MNATB ="MNAT", NAMTB = "MNAT",
NMNT = "NANT", NANTB ="NANT", JULNB ="NANT",NMNTB ="NANT",
LSHB= "SHB", SHBB = "SHB", SHRBB = "SHB", SHRB = "SHB",
LWSG = "WSG", WSGB = "WSG", WS ="WSG", WSB = "WSG", WSDB = "WSG") #TEST = "LAB2"
band_defns <- tribble(
~definition, ~color, ~min, ~max,
"ITEX", "red", 560, 600,
"ITEX", "nir", 725, 1000,
"MODIS", "red", 620, 670,
"MODIS", "nir", 841, 876,
"MODIS", "blue", 459,479,
"SKYE", "red", 620, 680,
"SKYE", "nir", 830, 880,
"SKYE", "blue", 455, 480,
"ToolikGIS_Drone_2018", "red", 640, 680,
"ToolikGIS_Drone_2018", "nir", 820, 890,
"ToolikGIS_MicaSense_2019", "blue", 455, 495,
"ToolikGIS_MicaSense_2019", "green", 540, 580,
"ToolikGIS_MicaSense_2019", "red", 658, 678,
"ToolikGIS_MicaSense_2019", "red_edge", 707, 727,
"ToolikGIS_MicaSense_2019", "near_ir", 800, 880,
"ToolikEDC", "red", 560, 680,
"ToolikEDC", "nir", 725, 1000
)
getwd()
## Missing ground value for S, C, 9.3 (GROUND = 0)
data.surface %>% arrange(desc(surface))
require(tidyverse)
data.og <- read_csv("point_frame_2019.csv")
require(tidyverse)
select <- dplyr::select
summarise <- dplyr::summarise
data.og <- read_csv("point_frame_2019.csv")
