---
title: "Read and Clean LTER Unispec Data"
author: "Ruby An"
date: "December 17, 2018"
output:
  html_document:
    df_print: paged
    number_sections: true
  html_notebook:
    number_sections: true
editor_options:
  chunk_output_type: console
---

This R Markdown document walks through reading in and cleaning historic (already collected) unispec data from 2010-2018. It is designed to be run through once for each “YEAR”. 


Run the code in the set-up chunk to load the required packages, functions, and vectors used in the reading and cleaning process.

```{r setup, echo=F}
knitr::opts_chunk$set(echo = FALSE, eval = FALSE, include = FALSE)

## Required Packages
library("tidyverse")
library("knitr")
require(lubridate)
require(openxlsx)
require(rstudioapi)
source("unispec_record_functions.R") # file loads required functions

## Standardize Names 
# Recode Site to standard names. This should cover must years.
Site_Names <- list(DH ="HTH", LHTH = "HTH", HTHB = "HTH", HTHPC = "HTH",
                   HIS="HIST", 
                   LOF = "LMAT",  LOFB = "LMAT", LNB = "LMAT", LOFRB ="LMAT",
                   MATB="MAT", MATSL= "MAT", MATBK = "MAT", 
                   MANTB ="MNAT",MNATB ="MNAT", NAMTB = "MNAT", 
                   NMNT = "NANT", NANTB ="NANT", JULNB ="NANT",NMNTB ="NANT",
                   LSHB= "SHB", SHBB = "SHB", SHRBB = "SHB", SHRB = "SHB", 
                   LWSG = "WSG", WSGB = "WSG", WS ="WSG", WSB = "WSG", WSDB = "WSG")
```

# Read Historic Data 

Read in “processed” .xlsx summary files and “raw” .spu files into R and standardize in the following 4 files: 

  * YEAR_raw_spu_data.rds
  * YEAR_raw_spu_key.csv
  * YEAR_xlsx_data.rds
  * YEAR_xlsx_key.csv

These 4 standardized files can be quickly read-in for subsequent cleaning and analysis to avoid repeatedly reading in many excel and .spu files.


## Choose Directory
Select the YEAR directory containing the files you want to process. Run the following code chunk interactively in RStudio to set folder via pop-up window. This document is set up to run through one year at a time.

```{r directory, eval=T}
if (interactive()) {
  ## INTERACTIVE CODE (use when in RStudio)
  data_path <- selectDirectory(caption = "Select Unispec files directory")
  dir_year <- str_split(data_path, pattern = "/")[[1]] %>% last()
} else {
  ## STATIC CODE (use when Knitting)
  data_path  <- "UnispecRecord/UnispecData/"
  dir_year <- "2014"
}
```

**Chosen Directory**: `r data_path`

## Read .xlsx summary files
```{r, read_xlsx, eval=FALSE}


## Unnest so that each row is a .spu file
key_info <- key_info_list %>% 
  unnest(file_contents) 

## 2012-2013 Specific Code:
# Get just the Spectra sheet where the spu file names are not all in the "Spectra" sheet. true for 2012,2013
# if (any(is.na(key_info$Spufilename))){key_info <- map_dfr(file_names,function (x)read.process_spectrafile (x), .id = "process_file")

# > Standardize Site Names --------------------------------------------------------
# Check for different spelling of site names
key_info$Site <- toupper(key_info$Site)
unique(key_info$Site)

key_info$Site <- recode (key_info$Site, !!!Site_Names, .default = key_info$Site)

# Recheck Site Names
unique(key_info$Site)
```

### Save .xlsx data & key
```{r, xlsx_dataframe, dependson="key_info"} 

# > Extract XLSX data--------------------------------------------------------

multispec_data <- key_info %>% select(spu_filename, xlsx_filename, ref_filenames, multispec_spectra) %>% unique() %>% 
  filter(! multispec_spectra %>% map(is.null) %>% map_lgl(any)) # remove files will no multispec spectra

## Confirm no duplicates: 
paste0("Duplicate multispec spectra: ", 
multispec_data %>% group_by(spu_filename) %>% filter(n()>1) %>% nrow() > 0)
duplicat_spu_filename <-key_info %>% group_by(spu_filename) %>% filter(n()>1) %>% arrange(spu_filename)
## Extract Index Data
index_data <- key_info %>% 
  select(-multispec_spectra) %>% # can't nest w/list column -- remove for now 
  gather(key = Index, value = Value, 'NDVI(MODIS)':'ChlIndex') %>% 
  filter(!is.na(Value)) %>% # remove rows w/out index info 
  nest(Index, Value, .key = "Indices") %>% 
  select(spu_filename, Indices) 

## Join multispec & index data
xlsx_data <- full_join(multispec_data, index_data) 

# > Extract XLSX Key --------------------------------------------------------
xlsx_key <- key_info %>% 
  arrange(spu_filename, Date) %>% 
  select(xlsx_filename, spu_filename, Date, Site, Block, Treatment, Replicate, Weather)


# > Save XLSX info --------------------------------------------------------
xlsx_filename <- paste0(data_path,"/", format(key_info$Date[1], format="%Y"),"_processed_xlsx_data.rds")
write_rds(xlsx_data, file = xlsx_filename)

xlsx_keyname <- paste0(data_path,"/", format(xlsx_key$Date[1], format="%Y"),"_processed_xlsx_key.csv")
write_csv(xlsx_key, file = xlsx_keyname)


```


## Read .spu raw files
```{r, spu_dataframe}
raw_data_path <- rstudioapi::selectDirectory(caption = "Select Raw Data Files Directory")
spu_files <- list.files(path = raw_data_path, pattern = ".spu$", full.names = T, recursive=T)

# Read all the metadata from the spu files using function read_spufile_metadata; add a variable with the filename
#  and max of ChA and ChB.  
spu_metadata <- map_dfr(spu_files, read_spu_file_metadata) %>% 
  mutate(spu_filename_full = spu_files)

spu_data <- spu_metadata %>% 
  mutate(Raw_Spectra=map(spu_filename_full, function(x) read_spu_file_spectra(x)))

# > Standardize Site Names -----------------------------------------------------
# Check for different spelling of site names
unique(spu_data$Site)
spu_data$Site <- recode (spu_data$Site, !!!Site_Names, .default = spu_data$Site)

# Recheck Site Names
unique(spu_data$Site)

#spu_dataframe <- read_rds("UnispecData/2016_raw_spu_dataframe.rds") %>% 
 spu_dataframe <- spu_data %>% 
    select(spu_filename, Site, FileNum, Date, DateTime, Integration_ms, Temp, Remarks, Raw_Spectra)

# > Set Type of Scan -----------------------------------------------------
spu_dataframe <- spu_dataframe %>% 
  mutate(Type = ifelse(grepl("DARKscan",Remarks, fixed=T), "Darkscan",
                       ifelse(grepl("Datascan,DC",Remarks, fixed=T), "Throwawayscan", NA))) %>% 
  distinct(DateTime, spu_filename, .keep_all = T)  # duplicate files in 2015 WSG 2014-06-20

# > Extract only Raw_Spectral data without metadata -----------------------------------------------------

spu_spectra <- spu_dataframe %>% select(spu_filename, DateTime, Raw_Spectra)

# > Extract only Raw_Spectral Metadata for .csv key -----------------------------------------------------
spu_key <- spu_dataframe %>% 
  select(-Raw_Spectra) %>% # remove for .csv
  select(spu_filename, everything()) # set order
```

### Save .spu data & key 
```{r, save_raw_data}

# > Extract only Raw_Spectral data without metadata -----------------------------------------------------
spu_spectra <- spu_dataframe %>% select(spu_filename, DateTime, Raw_Spectra)

## Save raw data as .rds 
raw_filename <- paste0(data_path,"/", format(spu_dataframe$DateTime[1], format="%Y"),"_raw_spu_data.rds")
write_rds(spu_spectra, file = raw_filename)


# > Extract only Raw_Spectral Metadata for .csv key -----------------------------------------------------
spu_key <- spu_dataframe %>% 
  select(-Raw_Spectra) %>% # remove for .csv
  select(spu_filename, everything()) # set order

## Save raw metadata key
raw_keyname <- paste0(data_path,"/", format(spu_dataframe$DateTime[1], format="%Y"),"_raw_spu_key.csv")
write_csv(spu_key, file = raw_keyname)

```


## Create Unispec Key  
Join .spu and .xlsx data and keys to create a standardized key used for quality check and quality control: 

  * YEAR_unispec_key.csv
  
The `*unispec_key.csv` file contains the information that should be noted in the RA notebook upon time of measurement. In the case of the “unispec record”, this info is extracted from the .xlsx summary files. This file can be manually edited using your text editor of choice (e.g. Excel) to fix MISLABELING errors, identified by running the following “QUALITY CHECK DATA” section. If any changes are made, they should be entered in the “key_fix” column. Possible entries include:

  * mislabeled : incorrect metadata for Site, Treatment, Replicate, Type, etc. 
  * unlabeled : the .spu file is not in the .xlsx summaries -- metadata missing 
  * TRUE / OTHER: some other fix -- any value other than “NA”  indicates the metadata for the spu file was originally wrong but is now corrected. 

A longer explanation of each mislabeling problem, if necessary, should be added to the “Notes” column. 
### Joined data & key
```{r}

# Join Processed & Raw .spu data -----------------------------------------------

# Joint metadata to spu data and use the Remarks variable to find the darkscan, and throw away scans and use the Reflectance mean to find References scans. This will help check the metadata. Not all spu files were used in the process files and some years inculde the used spu files in the the raw folder.

### Join Data
unispec_data <- spu_key %>% select(-Date) %>% rename(Site_filename = Site) %>% # raw metadata
  full_join(spu_spectra) %>% # add raw spectra
  full_join(xlsx_key, by = "spu_filename") %>% # add .xlsx metadata
  full_join(xlsx_data) %>% # add multispec_spectra and Indices
  arrange(DateTime) %>% 
  ## Fix File Numbers 
  mutate(FileNum = FileNum %% 100000) %>%  # correct for large filenumbers (>5 digits due to number at end of Site name)
  ## Fill in missing Site info, first xlsx key metadat, if NA then from spu_filename
  mutate(Site = coalesce(Site, Site_filename)) %>% 
  ## Fill in missing Date info: first xlsx key metadata, if NA then from spu_filename 
  mutate(Date = coalesce(Date, lubridate::date(DateTime) )) %>% 
  ## Convert Block from number to character 
  mutate(Block = ifelse(Block %in% c("1", "2", "3", "4"), str_c("B", Block), Block)) 

### Create file key (original labels, from .xlsx, before fixes) 
unispec_key <- unispec_data %>% 
  # SELECT Variables
  select(spu_filename, xlsx_filename, Type, Date, Site, Block, Treatment, Replicate, FileNum, Weather) 
```

### Save Unispec key
```{r, dependson="directory"}
## Save .csv of file key for manual editing 
unispec_keyname_og <- paste0(data_path,"/", format(unispec_key$Date[1], format="%Y"),"_unispec_key.csv")
write_csv(unispec_key, path = unispec_keyname_og)
```


# Load Saved Data
You need only run through the "READ" sections once per "YEAR" directory. Then for further cleaning, you can skip to here to load the .rds data and .csv key files directly.

### Load .xlsx Data
Includes multispec spectral data and index data calculated using .xlsx summary sheets.

  - 2014 : "UnispecData/2014_processed_xlsx_data.rds"; "UnispecData/2014_processed_xlsx_key.csv"
  - 2015 : "UnispecData/2015_processed_xlsx_data.rds"; "UnispecData/2015_processed_xlsx_key.csv"
  - 2016 : "UnispecData/2016_processed_xlsx_data.rds"; "UnispecData/2016_processed_xlsx_key.csv"
  - 2017 :
  - 2018 :

```{r, load_xlsx_data, eval = T}
xlsx_filename <- paste0(data_path, "/", dir_year, "_processed_xlsx_data.rds")
xlsx_data <- read_rds(xlsx_filename)

multispec_data <- xlsx_data %>% select(spu_filename, xlsx_filename, ref_filenames, multispec_spectra,Indices)
index_data <- xlsx_data %>% select(spu_filename, xlsx_filename, Indices)

xlsx_keyname <- paste0(data_path,"/", dir_year,"_processed_xlsx_key.csv")
xlsx_key <- read_csv(xlsx_keyname)

df<- xlsx_key %>% mutate(Site_filename = str_replace_all(spu_filename,c("jun"="","aug"="","jul"="" ,".spu"="","[:digit:]"=""))) %>%  
mutate(FileNum = as.numeric(str_match(spu_filename, "(\\d{4,5}).spu")[,2]))
df$Site_filename <- toupper(df$Site_filename)
df$Site_filename <- recode (df$Site_filename, !!!Site_Names, .default = df$Site_filename)
write.csv(df,file = paste0(data_path,"/",dir_year,"xlsx_key.csv"))

xlsx_key_data <- full_join(xlsx_key,xlsx_data)

```

### Load .spu raw data 
Includes raw spectra and instrument metadata.  

  - 2014 : "UnispecData/2014_raw_spu_data.rds"; "UnispecData/2014_raw_spu_key.csv"
  - 2015 : "UnispecData/2015_raw_spu_data.rds"; "UnispecData/2015_raw_spu_key.csv"
  - 2016 : "UnispecData/2016_raw_spu_data.rds"; "UnispecData/2016_raw_spu_key.csv"
  
```{r, load_raw_data, eval = T}
raw_filename <- paste0(data_path,"/", dir_year, "_raw_spu_data.rds")
spu_data <- read_rds(raw_filename)

raw_keyname <- paste0(data_path,"/",  dir_year,"_raw_spu_key.csv")
spu_key <- read_csv(raw_keyname) 

```

### Load Unispec Key Fix and Dataframe
Includes manually corrected metadata. 

  - 2014 : "UnispecData/2014_unispec_key.csv"
  - 2015 : "UnispecData/2015_unispec_key.csv"
  - 2016 : "UnispecData/2016_unispec_key.csv"  
  

```{r, df, dependson=c("load_raw_data", "load_xlsx_data"), eval=TRUE}
unispec_keyname <- paste0(data_path,"/", dir_year, "_unispec_key.csv") # manually updated key
unispec_key_fix <- read_csv(unispec_keyname, skip_empty_rows = T, col_types = cols(
  spu_filename = col_character(),
  Type = col_factor(),
  Date = col_date(format = ""),
  Site = col_factor(),
  Block = col_factor(),
  Treatment = col_factor(),
  Replicate = col_double(),
  FileNum = col_integer(),
  Weather = col_character(),
  Notes = col_character(),
  key_fix = col_factor()
)) %>% 
  filter(!is.na(spu_filename))
# problems for 2015: 100 EXTRA files, not needed. 

## Dataframe w/fixed metadata
df <- left_join(unispec_key_fix, spu_data) %>% # Spu Spectra
  left_join(spu_key %>% select(-Site, -FileNum, -Date, -Remarks, -Type)) %>% # Integration info
  left_join(multispec_data) %>% arrange(DateTime)

```


**RAW SPU SUMMARY FILES**: 

  - `r raw_filename`
  - `r raw_keyname`

**XLSX SUMMARY FILES**: 

  - `r xlsx_filename`
  - `r xlsx_keyname`
  
**UNISPEC KEY FILE**:

  - `r unispec_keyname`


# Quality Check Quality Control 

## Zoom Check
Run the following code chunk interactively in RStudio to check spu_files and metadata at specific sites/dates. 
```{r plot_zoom, eval=T, echo=F, dependson="df"} 

## SPECIFY SITE/DATE/ETC to ZOOM IN ON
check_sites <- c("LMAT")

## Possible Dates
check_dates <- df %>% filter(Site %in% check_sites) %>% pull(Date) %>% unique()

## Which Date
check_dates <- c("2016-07-21") # necessary to unlist dates vector

df_check <- df %>% # full dataframe not just ref's 
  filter(format(DateTime, format="%Y-%m-%d") %in% check_dates) %>% # Date is NA for spu_files without 
  filter(Site %in% check_sites) %>% 
  arrange(DateTime)  %>% 
  select(spu_filename, DateTime, Site, Block, Treatment, Replicate, FileNum, Type, Raw_Spectra, everything())

## PLOT RAW DATA --for specific File Numbers
start_file <- 0
end_file <- 10

df_check_files <- df_check %>% 
  filter(FileNum >= start_file, FileNum <= end_file) 

df_check_files  %>%  plot_channels()


# # PLOT METADATA
# df_subset <- df_check %>% select(spu_filename) %>% inner_join(df)
# df_subset %>%
#   slice(1:10) %>%
#   plot_reflectances()
# 

## TIME CHECK
df_check %>% check_time_difference() %>% mutate(difference = diff) %>%  select(FileNum, difference, Treatment, Replicate, everything()) %>% print(n=100)
```


## Mis-Labeling 


### Discrepancy Checks
```{r df_check, dependson="df", eval= T, echo=F}
##### ------------------------- Variable checks 

# Check variables
df_names <- names(df)

#####
# Check for Meaningful NA's 
df_na <- df %>% filter(is.na(spu_filename) |
              is.na(Site) |
              is.na(Treatment) & !is.na(xlsx_filename) | # missing treatment in .xlsx file
              is.na(Block) & !str_detect(Treatment, "REF") | # Block NA's should always be REFS or EXTRA
              is.na(Replicate) & !str_detect(Treatment, "REF"), # Check for replicate NA's that aren't REF
              Treatment != "EXTRA") # don't care about EXTRA
df_na

#####
## Check for large filenumbers : Inconsistent File Number reading due to number at end of site names
df_lgfn <- df %>%
  filter(FileNum > 500) %>%  # Sites w/numbers at the end mis-read in as part of file number. All > 100000: Unispec-DC measurements only go up to 5 digits.
  select(Date, Site, Block, FileNum, spu_filename)
df_lgfn


#####
## Check Site Names
df_sites <- unique(df$Site)
df_sites

## Check Treatment names
df_treatments <- unique(df$Treatment)
df_treatments

### -------------------------------------- SUMMARY CHECKS

## Dataframe 
df_summary <- df %>% group_by(Site, Date) %>%  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), Num_Files = n()) 
df_summary %>% print(n=100)

# Discrepancies  
## Site vs. Site_filename differences 
#### should only be Reference files or mislabeled (e.g. MAT extends into LMAT, or NANT continues to MNAT)
df_site_diff <- unispec_key_fix %>% left_join(spu_key %>% select(-Date, -Type, -FileNum) %>% rename(Site_filename = Site)) %>%   filter(Site != Site_filename) 

df_site_diff %>% 
  group_by(Date) %>% 
  summarize(Sites = str_c(unique(Site), collapse = ", "), Sites_filename = str_c(unique(Site_filename), collapse = ", "), Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), Num_Files = n())

## Check File Number Pattern:
#### Num Files per block should be multiples of 5, unless REF or NA
df_filenum_count <- unispec_key_fix %>% group_by(Site, Date, Block) %>% 
  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), Num_Files = n()) %>% 
  filter(Treatments != "NA") %>% 
  filter(Num_Files %% 5 != 0) %>% # files per plot
  filter(!str_detect(Treatments, "REF|NA")) 
df_filenum_count

df %>% inner_join(df_filenum_count) %>% print(n=100) 

```

Run the following code for Discerpancy check output. 
```{r, checks, eval = T, echo=F}

# cat(paste0("Dataframe Columns: ", str_c(df_names, collapse = ", ")))
# cat(paste0("Sites: ", str_c(df_sites, collapse = ", ")))
# cat(paste0("Treatments: ", str_c(df_treatments, collapse = ", ")))
# 
# df_na %>% kable(caption = "NA Metadata")
# df_lgfn %>% kable(caption = "Large FileNumbers")

df_names
df_sites
df_treatments

df_lgfn 
df_site_diff
df_filenum_count

df_summary

### Summarize  - look for NA's
unispec_key_fix %>% select_if(function(x) typeof(x) != "list") %>% summary()
df %>% select_if(function(x) typeof(x) != "list") %>% summary()

```

### Duplicates 
Identify duplicates. Fix by editing unispec_key_fix.csv. Only duplicates should be REF files used for other sites. 

```{r duplicates}
### Check raw spu_files have no duplicates
nrow(spu_data) == length(unique(spu_data$spu_filename))

## Check for duplicates in key
unispec_key_fix  %>% group_by(spu_filename) %>%
  filter(n()>1) %>%
  arrange(spu_filename) %>% 
  filter(!(Site %in% c("LMAT", "MAT") & Date == "2015-07-10")) %>% # file naming error in 2015
  print(n=100)

## Check for duplicate in dataframe
duplicates <- df %>% 
  group_by(spu_filename) %>%
  filter(n()>1) %>%
  ungroup() %>%
  arrange(DateTime, spu_filename) %>% 
  select( xlsx_filename, spu_filename, FileNum, DateTime, Site, Block, Treatment, Replicate, Weather, everything())

# duplicates %>% print(n=200)
# duplicates %>% filter(!str_detect(Treatment, "REF")) %>% arrange(spu_filename) %>% print(n=100)

duplicates %>% 
  mutate(Date = coalesce(Date, lubridate::date(DateTime))) %>% 
  group_by(Date) %>% 
  summarize(Sites = str_c(unique(Site), collapse = ","), Treatments = str_c(unique(Treatment), collapse = ","), Files  = n()) %>% 
  kable()

```

Edit `*unispec_key.csv` to resolve discrepancies and any duplicates. 

### Mislabeled files 
```{r mislabeled}
## Mislabeled files 
mislabeled_files <- df %>% filter(!is.na(key_fix)) %>% pull(spu_filename)
```


## Missing data 
List missing raw .spu files and those that are unprocessed (raw .spu file does not appear in summary .xlsx). 

### Missing raw files
```{r missing}
### MISSING: Find all processed data that is missing corresponding raw spu files 
missing_spu_data <- anti_join(unispec_key_fix, spu_data) 

missing_spu_data %>% group_by(Date, Site) %>% 
  summarize(Treatments = str_c(unique(Treatment), collapse = ","), Files = n()) %>% 
  kable()

missing_spu_files <- missing_spu_data$spu_filename
```

### Unprocessed raw files
```{r unprocessed}
### UNPROCESSED: Find all unprocessed spu files (missing multispec data, not listed as a reference)

xlsx_refs <- xlsx_key %>% filter(str_detect(Treatment, "REF")) %>% select(spu_filename) %>% pull()

unprocessed_spu_data <- df %>% 
  filter(!spu_filename %in% xlsx_refs) %>% 
  filter(is.na(xlsx_filename)) %>% arrange(spu_filename)

# Summary
unprocessed_spu_data %>%  group_by(Date, Site) %>% 
  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), Files = n()) %>% 
  kable()

unprocessed_spu_files <- unprocessed_spu_data$spu_filename

```


## Mis-Measurement

### Max'd Out Spectra (> 65000 AD)
List files that maxed out, in the wavelengths used to calculate MODIS NDVI.
```{r maxed, dependson="df"}

maxed_data <- df %>% inner_join(spu_data %>% select(spu_filename)) %>% unnest(Raw_Spectra) %>% 
  filter(ChA > 65000 | ChB > 65000) %>% 
  group_by(spu_filename) %>% 
  summarize(maxed_number = n(), maxed_wavelengths = str_c(Wavelength, collapse = ", ")) 

maxed_files <- maxed_data$spu_filename

## Summary of Maxed files 
maxed_data %>% inner_join(df) %>% group_by(Site, Date) %>% 
  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), Num_Files = n(), 
            Num_Maxed = round(mean(maxed_number), digits = 1)) %>%
  kable()

## Select those Max'd in NDVI Region
maxed_limit <- 5 # keep files where spectra is max'd only at a narrow peak (e.g. for maxed_limit = 5, maxed region < 5*3.3nm = 16.5 nm)

maxed_data_bad <- df %>% inner_join(spu_data %>% select(spu_filename)) %>% unnest(Raw_Spectra) %>% 
  filter(ChA > 65000 | ChB > 65000) %>% 
  ## Extra conditions for NDVI region
  filter(Wavelength > 620) %>% # MODIS red lower bound
  filter(Wavelength < 876) %>% # MODIS nir upper bound
  ## Summary
  group_by(spu_filename) %>% 
  summarize(maxed_number = n(), maxed_wavelengths = str_c(Wavelength, collapse = ", ")) %>% 
  filter(maxed_number > maxed_limit)

## Summary of NDVI Max'd files 
maxed_data_bad %>% inner_join(df) %>% group_by(Site, Date) %>% 
  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), Num_Files = n(), 
            Num_Maxed = round(mean(maxed_number), digits = 1)) %>% 
  print(n=50)

maxed_files_bad <- maxed_data_bad$spu_filename
```


### Dim'd Out Spectra
Primarily Darkscans should show up. 
```{r dim, dependson=df}
dim_data <- df %>% inner_join(spu_data %>% select(spu_filename)) %>% unnest(Raw_Spectra) %>% 
  group_by(spu_filename) %>% 
  summarize(ChA_max = max(ChA)) %>% 
  filter(ChA_max < 20000) %>% 
  left_join(df) 

# Summary of important info
dim_data %>% group_by(Date, Site) %>% 
  # filter(!is.na(Treatment)) %>% 
  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), 
            Types = str_c(str_replace_na(unique(Type)), collapse = ","), 
            Files = n(), ChA_Max = max(ChA_max))  %>% 
  kable()

# File List
dim_files <- dim_data$spu_filename
```


### Absurd Reflectance > 1

#### Zero'd Spectra
```{r zerod}
## zero'd data 
zero_data <- df %>% inner_join(spu_data %>% select(spu_filename)) %>% unnest(Raw_Spectra) %>% 
  filter(ChA == 0) %>%  ## This is for all wavelengths, not just 400-1000nm
  group_by(spu_filename) %>% 
  summarize(Zeros = n()) %>% 
  left_join(df)
  
## Summary 
zero_data %>% group_by(Date, Site) %>% 
  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), 
            Files = n(),
            Max_Zeros = max(Zeros)) %>% 
  kable()


## File List : Restricted Wavelengths
zero_files <- zero_data %>% unnest(Raw_Spectra) %>% 
  filter(ChA == 0 ) %>% 
  filter(Wavelength > 400, Wavelength < 1000) %>% 
  pull(spu_filename) %>% unique()

zero_data_narrowed <- df %>% filter(spu_filename %in% zero_files) 

## Plot Exploration
zero_data_narrowed %>% 
  slice(1:5) %>% plot_channels()

## Summary
zero_data_narrowed %>% group_by(Date, Site) %>% 
  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), Files = n())  %>% 
  kable()
  
```

#### Reflectance >1 
```{r absurd}

## Raw Spectra
absurd_raw_data <- df %>% 
  filter(! Raw_Spectra %>% map(is.null) %>% map_lgl(any)) %>% # remove files w/out spu_spectra
  filter(!str_detect(Treatment, "REF")) %>% # ignore REF files
  unnest(Raw_Spectra) %>% # use raw spectra
  filter(Wavelength > 400 & Wavelength < 1000) %>% 
  filter(Reflectance > 1) %>% 
  nest(Wavelength, ChA, ChB, Reflectance, .key = Raw_Spectra)

absurd_raw_files <- absurd_raw_data$spu_filename

absurd_raw_data %>% group_by(Date, Site) %>% 
  summarize(Treatments = str_c(unique(Treatment), collapse = ","), Files = n())

## Corrected Spectra
absurd_data <- df %>% 
  filter(!str_detect(Treatment, "REF")) %>% # ignore REF files
  filter(!spu_filename %in% unprocessed_spu_files) %>% # leave out missing multispec data
  unnest(multispec_spectra) %>% # use corrected spectra
  filter(Wavelength > 400 & Wavelength < 1000) %>% 
  filter(Reflectance > 1) %>% 
  nest(Wavelength, Reflectance, .key = multispec_spectra)

## Summary
absurd_data %>% group_by(Date, Site) %>% 
  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), Files = n())  %>% 
  kable()

## File List 
absurd_files <- absurd_data$spu_filename

## Plot Check 
absurd_data %>% 
  inner_join(spu_data) %>% 
  slice(1:10) %>% 
  plot_channels()

```


## Mis-Correction
White references correct for instrument & cable irregularities. Multiplying by the correction factor (ChA/ChB) smooths out the spectra. If multiple file numbers are listed (typically 5), the correction factors are averaged.

### White References
The following code checks white reference files listed in the .xlsx key and identifies sets of .spu files with problematic References. 

```{r refs, dependson="df"}
## Identify Site/Dates with missing REF's
print("Sites/Dates with missing References")
df  %>% group_by(Site, Date) %>%  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), Num_Files = n()) %>% 
  # filter(Treatments != "NA") %>% 
  filter(!str_detect(Treatments, "REF")) %>% #if non-empty, then REF files missing for some Date, Site
  kable()

## Find all white reference data files (for MULTISPEC)
ref_data <- xlsx_key %>% 
  filter(str_detect(Treatment, "REF")) %>%  # extract reference data
  inner_join(spu_data)

## Build Plot all reference data files
ref_data_all <- ref_data %>% unnest(Raw_Spectra) %>%
  filter(Wavelength > 400, Wavelength < 1000) %>% 
  mutate(CorrectionFactor = 1/Reflectance)

ref_plot_all <- ggplot(ref_data_all, aes(x = Wavelength, y = CorrectionFactor)) +
  geom_line(aes(color=spu_filename)) + theme(legend.position="none")

## Build Plot all reference mistakes
ref_data_mistakes <- ref_data_all %>%
  filter(CorrectionFactor > 5) %>% 
  distinct(spu_filename) %>% 
  select(spu_filename) %>% 
  left_join(ref_data_all)

ref_plot_mistakes <- ggplot(ref_data_mistakes, aes(x = Wavelength, y = CorrectionFactor)) +
  geom_line(aes(color=spu_filename))

## Summary
ref_data %>% group_by(Date, Site) %>% 
  summarize(Treatments = str_c(unique(Treatment), collapse=","), Files = n_distinct(spu_filename)) %>% 
  kable()

## PLOTS
#ref_plot_all + ggtitle("ALL REFERENCES")
ref_plot_mistakes + ggtitle("REF Mistakes") +
  scale_y_continuous(limits = c(0, NA))

## File Lists
ref_files <- ref_data$spu_filename %>% unique()
ref_mistakes <- ref_data_mistakes %>% distinct(spu_filename) %>% pull()
```

Look for correction factors very far from 1.0 or with odd peaks. Edit unispec_key_fix.csv file to resolve errors as you go.


# Label Unispec Problems 

## Problem Key 
```{r unispec_problem_key}

unispec_problem_key_pre <- unispec_key_fix %>% 
  # mislabeled 
  mutate(mislabeled = !is.na(key_fix)) %>% # mislabeled 
  # mismeasurement
  left_join(maxed_data) %>% # maxed_number, maxed_wavelenghs
  mutate(maxed = maxed_number > 5) %>% # choose how strict to make this using maxed_number threshold, etc. # mutate(maxed = spu_filename %in% maxed_data_files_bad) %>% 
  mutate(dim = spu_filename %in% dim_files) %>% # dim
  mutate(absurd_reflectance = spu_filename %in% c(zero_files, absurd_files)) %>% # absurd_reflectance
  # missing 
  mutate(missing_spu = spu_filename %in% missing_spu_files) %>%  # missing_spu
  mutate(missing_xlsx = spu_filename %in% unprocessed_spu_files) %>% # unprocessed_spu
  left_join(multispec_data %>% select(-multispec_spectra)) %>%  # add ref_filenames
  mutate(ref_filenames = tolower(ref_filenames)) # make sure all filenames are lower case
  
# miscorrection 
ref_problem_key <- unispec_problem_key_pre %>%
  filter(spu_filename %in% ref_files) %>% # select ref files 
  #mutate(mislabeled = if_else(spu_filename %in% ref_mistakes, TRUE, mislabeled)) %>% 
  gather("problem", "status", mislabeled, maxed, dim, absurd_reflectance, missing_spu) %>% # row for each type of problem
  filter(!is.na(status)) %>% # remove non-problems
  filter(status != FALSE) %>% # remove non-problems
  group_by(spu_filename) %>% 
  select(spu_filename, problem, status) %>% 
  summarize(problems = str_c(unique(problem), collapse = ", ")) %>% # group: one row per spu_filename
  rename(ref_filenames = spu_filename, ref_problem = problems) # rename variables to join w/unispec_problem_key

## Unispec Problem Key
unispec_problem_key <- unispec_problem_key_pre %>% 
  separate_rows(ref_filenames, sep = ", ") %>% # split ref_filenames string into multiple rows per ref_filename
  left_join(ref_problem_key) %>% # adds ref_problem column 
  group_by(spu_filename) %>% 
  summarize(ref_problems = str_c(ref_problem, collapse = "; ")) %>% # collapse to one row per spu_filename
  right_join(unispec_problem_key_pre) %>%  # add the rest of the key info back in
  mutate_at(.vars = c("ref_problems", "dim", "absurd_reflectance", "mislabeled", "missing_spu", "missing_xlsx"), .funs = factor) %>% 
  #mutate(file_problem = any(maxed, dim, absurd_reflectance)) %>% 
  mutate(file_problem = spu_filename %in% c(maxed_files_bad, dim_files, absurd_files)) %>% 
  select(spu_filename, Type:Weather, missing_spu, missing_xlsx, file_problem, mislabeled, key_fix, maxed, maxed_number, maxed_wavelengths, dim, absurd_reflectance, ref_problems, ref_filenames, xlsx_filename) # order

unispec_problem_key %>%
  mutate_at(funs(factor), .vars = vars(c("Site", "Block", "Treatment", "key_fix", "ref_problems"))) %>% 
  mutate(file_problem = spu_filename %in% c(maxed_files_bad, dim_files, absurd_files))  %>% 
  summary() 
```

### Save Unispec Problem Key
All the information you could ever want in one .csv! 
```{r}
## Save .csv of file key
unispec_problem_keyname <- paste0(data_path, format(unispec_problem_key$Date[1], format="%Y"),"_unispec_key_problems.csv")

## update file_problem column
unispec_problem_key <- unispec_problem_key %>%   mutate(file_problem = spu_filename %in% c(maxed_files_bad, dim_files, absurd_files))

write_csv(unispec_problem_key, path = unispec_problem_keyname)
```

### Load Problem Key
Full Unispec Problem Keys have been saved for the following years:
  - 2014 : "UnispecData/2014_unispec_key_problems.csv"
  - 2015 : "UnispecData/2015_unispec_key_problems.csv"
  - 2016 : "UnispecData/2016_unispec_key_problems.csv"
  - 2017 :
  - 2018 :
```{r}
key_filename <- paste0(data_path, dir_year, "_unispec_key_problems.csv") # manually updated key
unispec_problem_key <- read_csv(key_filename, skip_empty_rows = T, col_types = cols(ref_problems = col_character())) %>% 
  mutate_at(.funs = factor, .vars = vars("Site", "Block", "Treatment", "Replicate", "key_fix", "ref_problems")) 

unispec_problem_key %>% summary()
```

## Dataframe 

### Save full dataframe
```{r}
## Includes key and problems metadata
unispec_dataframe <- unispec_problem_key %>% 
  left_join(spu_data) %>% 
  left_join(spu_key %>% select(spu_filename, DateTime, Integration_ms, Temp)) %>% 
  left_join(multispec_data %>% select(spu_filename, multispec_spectra)) 

## Only the data  
unispec_data <- spu_key %>% 
  full_join(spu_data) %>% 
  full_join(multispec_data %>% select(spu_filename, multispec_spectra)) %>% 
  select(-Site, -FileNum, -Date, -Type) # duplicated // already present in unispec_key.csv files 

## save dataframe w/problem info 
unispec_filename <- paste0(data_path, format(unispec_data$DateTime[1], format="%Y"),"_unispec_data.rds")
write_rds(unispec_data, path = unispec_filename)
```


### Load Unispec Data
Standardized data has been saved for the following years:
  - 2014 : "UnispecData/2014_unispec_dataframe_problems.rds"
  - 2015 : "UnispecData/2015_unispec_dataframe_problems.rds"
  - 2016 :
  - 2017 :
  - 2018 :

```{r}
df <- read_rds(paste0(data_path, "2016_unispec_dataframe.rds"))

```
  
  
  