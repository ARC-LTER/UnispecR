---
title: "Process LTER Unispec Data"
author: "Ruby An"
date: "December 17, 2018"
output:
  html_document:
    df_print: paged
  html_notebook:
    number_sections: yes
editor_options:
  chunk_output_type: console
---

# How to use this R Markdown Notebook: 

This R Markdown document walks through processing historic (already collected) unispec data from 2017-2018. 

# Required Packages
```{r setup, echo=F}
knitr::opts_chunk$set(echo = TRUE)

## Required Packages
library("tidyverse")
library("knitr")
source("unispec_functions.R") # file loads required functions

## Useful vectors for standardizing names and filtering data
WSG <- c("WSG1", "WSG23")
SHB <- c("SHB1", "SHB2")
site_list <- list("MAT", "LMAT", "MNAT", "NANT", "DHT", WSG, SHB, "HST")

# Recode Site to standard names. This should cover must years.
Site_Names <- list(DHT = "HTH", DH ="HTH", LHTH = "HTH", HTHB = "HTH", HTHPC = "HTH", HST = "HIST", HIS="HIST", 
                   LOF = "LMAT",  LOFB = "LMAT", LNB = "LMAT", LOFRB ="LMAT",
                   MATB="MAT", MATSL= "MAT", MATBK = "MAT", 
                   MANTB ="MNAT",MNATB ="MNAT", NAMTB = "MNAT", 
                   NMNT = "NANT", NANTB ="NANT", JULNB ="NANT",NMNTB ="NANT",
                   LSHB= "SHB", SHBB = "SHB", SHRBB = "SHB", SHRB = "SHB", 
                   LWSG = "WSG", WSGB = "WSG", WS ="WSG", WSB = "WSG", WSDB = "WSG")

CT <- c("CT","CT1","CT2")
NP_gradient <- c("F0.5","F1","F2","F5","F10") # for LOF site
N_types <- c("NO3", "NH4") # for LOF site
trtmt_list <- list(CT, "N", "P", "NP", NP_gradient, N_types)

## Useful vectors for plotting
# Color sequences
pur_pal <- RColorBrewer::brewer.pal(5, "Purples")
```


# Choose Directory

Select the folder containing the unispec files you want to process. Run the following code chunk interactively in RStudio to set folder via pop-up window. 
```{r directory}
interact <- interactive() 

if (interact) { 
  ## INTERACTIVE CODE (use when in RStudio)
  library("rChoiceDialogs") # for interactively selecting file directories
data_path <- rchoose.dir(caption = "Select Unispec files directory")
} else { 
  ## STATIC CODE (use when Knitting)
  data_path  <- "UnispecData/2017" # local computer
  data_path <- "/mnt/c/Users/toolik/OneDrive - Marine Biological Laboratory/Toolik Terrestrial/UnispecData/2017/spu" # Terrerstrial RA computer 
}
```

**Chosen Directory**: `r data_path`

This directory should contain both the `.spu` files you wish to process and a corresponding `*_unispec_key.csv` file. The key file matches the .spu files to the date, site, block, treatment, plot, & measurement and specifies which white references to use to correct for instrument error. 

# Load Unispec Data 
Run the following code chunks to load & join keys to data from your chosen directory. 

## Read Keys 
Select unispec key files interactively or search within: `r data_path`. 

```{r key} 
if (interact) {
  key_files <- rchoose.files() # choose via window
} else {
  ## Find all file keys matching search pattern 
  key_files <- list.files(path = data_path, pattern = "*_key.csv", full.names = T, recursive = T)
} 

## Read in data from filekeys 
key_list <- tibble(keyname = key_files) %>% # create dataframe
  mutate(key_contents = map(keyname, function(x) read_key_file(x))) 
# read_key_file() is a function I wrote located in the file "unispec_functions.R"
# map function: super useful to apply function to objects without slow "for" loops

## Unpack into usable dataframe 
keys <- unnest(key_list)
```
**Chosen Keys**: `r key_files`

## Read Data
You can choose to select unispec data files interactively; however, it is usally more convenient to let R search: `r data_path` -- for all `.spu` files. This step may take several minutes, if you chose a folder containing many files.


```{r spu_data, cache =T}

if (interact) {
  files <- rchoose.files(filter=c("*.spu")) # choose via window
} else {
  ## Find .spu files via search pattern: specify Date(s) or Site(s) to read files
  files <- list.files(path = data_path, pattern = ".spu$", full.names = T, recursive=T)
} 

## Read data from files (can take minutes)
data_list <- tibble(filename = files) %>% # create dataframe
  mutate(file_contents = map(filename, function(x) read_spu_file(x)))

## Unpack into usable dataframe 
data <- unnest(data_list) %>% 
  filter(Wavelength >= 400 & Wavelength <= 1000) # trim unreliable wavelengths
```


The following table lists the dates and sites of the files contained in your chosen directory: 


## Load R files 

### Load .spu raw data 
Includes raw spectra and instrument metadata.  

  - 2017 : "UnispecData/2017_raw_spu_data.rds"; "UnispecData/2017_raw_spu_key.csv"
  
```{r, load_raw_data, eval = T}
raw_filename <- paste0(data_path, dir_year, "_raw_spu_data.rds")
spu_data <- read_rds(raw_filename)

raw_keyname <- paste0("UnispecData/", dir_year, "_raw_spu_key.csv")
spu_key <- read_csv(raw_keyname) %>% select(-DateTime) # reading error with DateTime zone

spu_dataframe <- full_join(spu_data, spu_key) %>% 
  #mutate(DateTime = ymd_hms(DateTime, tz = "US/Alaska")) %>% 
  mutate(Date = str_extract(spu_filename, pattern = "^[0-9]{4}-[0-9]{2}-[0-9]{2}")) # from filename, not DateTime (sometimes discrepancy in unispec instrument time)

### Load multispec data
multispec_data <- read_rds(paste0("UnispecData/", dir_year, "_multispec_data.rds"))
  
```

### Load Unispec Key 
```{r, df, dependson=c("load_raw_data", "load_xlsx_data"), eval=TRUE}

unispec_keyname <- paste0("UnispecData/", dir_year, "/", dir_year, "_unispec_key.csv") # manually updated key
unispec_key <- read_csv(unispec_keyname, skip_empty_rows = T) %>% 
  mutate(Date = lubridate::mdy(Date)) %>% 
  gather(key = Replicate, value = FileNum, P1:P5) %>% 
  filter(!is.na(FileNum)) %>% 
  mutate(Replicate = as.integer(str_remove(Replicate, pattern = "P"))) %>% 
  arrange(Date, Site, Block, FileNum) 

#Standardize site names
unispec_key$Site <- recode (unispec_key$Site, !!!Site_Names, .default = unispec_key$Site)
unispec_key$Site <- str_replace(unispec_key$Site, "DHT", "HTH") # fix HTH
unispec_key$Site <- str_replace(unispec_key$Site, "HST", "HIST") 

keys <- unispec_key
```


## Join Data & Keys
```{r join_keys_data}
## Dataframe w/fixed metadata
df <- full_join(spu_dataframe, unispec_key) %>% arrange(DateTime) %>% 
  left_join(multispec_data %>% filter(ProcessType == "correct") %>% select(Date, Site, FileNum, Block, Treatment, Replicate, multispec_spectra)) %>% arrange(DateTime) %>% 
  mutate_at(.vars = vars(Site, Block, Treatment, Replicate), .funs = factor)

## Standardize Site Names 
### Check
df %>% pull(Site) %>% unique()

## Sites per Date table to display
df_table <- df %>% 
  select(Date, Site) %>% 
  distinct() %>% 
  group_by(Date) %>% 
  do(Sites = t(.[-1])) %>% 
  mutate(Sites = paste( unlist(Sites), collapse=','))

kable(df_table)
```

### Save File Key

Edit this .csv to fix mislabeling errors 
```{r}
unispec_file_key <- df %>% 
  select(spu_filename, Type, Date, Site, Block, Treatment, Replicate, FileNum, Weather, Notes, key_fix)

write_csv(unispec_file_key, paste0("UnispecData/", dir_year, "_unispec_file_key.csv"))
```

### Load File Key Fix
```{r}
unispec_file_key <- read_csv(paste0("UnispecData/", dir_year, "_unispec_file_key.csv"), col_types = cols(
  spu_filename = col_character(),
  Date = col_character(),
  Site = col_character(),
  Block = col_character(),
  Treatment = col_character(),
  Replicate = col_double(),
  FileNum = col_double(),
  Weather = col_character(),
  Notes = col_character(),
  Type = col_character(),
  key_fix = col_logical()
))

df <- full_join(spu_dataframe, unispec_file_key %>% select(-spu_filename)) %>% arrange(DateTime) %>% 
  mutate_at(.vars = vars(Site, Block, Treatment), .funs = factor)
```


## White Reference Correction
White references correct for instrument & cable irregularities. Multiplying by the correction factor (ChA/ChB) smooths out the spectra. There are typically 5 reference measurements per *Date* / *Site*. If multiple file numbers are listed,, the correction factors are averaged. 

Based on the original field notebook key and the following quality checks, choose reference files by entering the appropriate file numbers in **`r key_files`** for the rows where the column *Treatment* = **REF**. 

Make sure to rerun the **Load Keys**. The following plots your chosen references.

### Extract REF Data
```{r ref_table, echo=F}
options(knitr.kable.NA = '')

## Build REF key 
ref_keys <- unispec_file_key %>% 
  filter(Treatment == "REF") %>% # extract reference data 
  ## The following separates the Site column into "Site" and "Site_REF"
  ### Site = the site to which the reference measurements should be applied 
  ### Site_REF = the site where the reference measurements were actually collected
  separate(Site, into=c("Site", "Site_REF"), sep = "_", fill="right") %>% 
  mutate(Site_REF = coalesce(Site_REF, Site)) # if the references were collected at 'Site', the created column Site_REF will be NA. Use coalesce() to fill these NA's with the value of "Site".  

### spu data for references
spu_for_ref <- spu_dataframe %>% # in "spu_dataframe", the "Site" column is the location where the data was collected 
  rename(Site_REF = Site) # we thus rename Site to Site_REF to match the column 'ref_keys'

## Join spu data to ref_keys by Site_REF, Date, FileNum
ref_data <- ref_keys %>% select(Date, Site, Site_REF, FileNum, Block, Treatment, Replicate, Weather, Notes, key_fix) %>% 
  left_join(spu_for_ref)  ## "Site_REF" is the location where the file (from which the reference correction factor is calculated) actually was collected
  ## "Site", inherited from ref_keys, is now the location where the correction factor should be applied

## Table of Reference Files
ref_table <- ref_data %>% group_by(Date, Site_REF) %>% 
  summarize(Sites = str_c(unique(Site), collapse = ","), Files = n_distinct(spu_filename)) %>% 
  kable()

## Table per Site of Reference Files 
ref_data %>% group_by(Date, Site, Site_REF) %>% 
  summarize(Files = n_distinct(spu_filename)) %>% 
  kable()

## Table per Site for all files 
df_ref_table <- df %>% 
  separate(Site, into=c("Site", "Site_REF"), sep = "_", fill="right") %>%
  filter(!is.na(Treatment)) %>% 
  group_by(Date, Site) %>% 
  summarize(Treatments = str_c(unique(Treatment), collapse = ","), Files = n_distinct(spu_filename))

#### Check that all Date / Sites have references 
df_missing_refs <- df_ref_table %>% 
  filter(!str_detect(Treatments, "REF")) %>%  
  print(n=100)

## Output
ref_table
df_missing_refs


```


### Plot References
```{r ref_choice_list}

## Build Plot all reference data files
ref_data_all <- ref_data %>% unnest(Spectra) %>%
  filter(Wavelength > 400, Wavelength < 1000) %>% 
  mutate(CorrectionFactor = 1/Reflectance)

ref_plot_all <- ggplot(ref_data_all, aes(x = Wavelength, y = CorrectionFactor, group=spu_filename)) +
  geom_line(aes(color=Integration_ms)) + theme(legend.position="none")


## Build Plot all reference mistakes
ref_data_mistakes <- ref_data_all %>%
  filter(CorrectionFactor > 5) %>% 
  distinct(spu_filename) %>% 
  select(spu_filename) %>% 
  left_join(ref_data_all)

ref_plot_mistakes <- ggplot(ref_data_mistakes, aes(x = Wavelength, y = CorrectionFactor)) +
  geom_line(aes(color=spu_filename))


## PLOTS
ref_plot_all + ggtitle("ALL REFERENCES")
ref_plot_mistakes + ggtitle("REF Mistakes") +
  scale_y_continuous(limits = c(0, NA))

## File Lists
ref_files <- ref_data$spu_filename %>% unique()
ref_mistakes <- ref_data_mistakes %>% distinct(spu_filename) %>% pull()

```

### Apply References
Rerun the **Join Data & keys** sections above to update the `df_clean` dataframe.Apply your chosen references to actual spectral data to create the tidy dataframe `df_tidy` containing corrected sepectral reflectance values.

```{r apply_refs, echo=FALSE}
## Average 5 chosen ref measurements per DATE/SITE/BLOCK 
ref_summary <- ref_data %>% 
  ## The following steps expand the "Block" column to create one REF set per Block per Site. This structure is necessary for situtations where different refs are used for different blocks at the same site. 
  separate(Block, into = c("BX1", "BX2", "BX3", "BX4"), sep = ",") %>% #1: expand string entry in "Block" into separate columns -- NOTE: this step throws a "Warning: Expected 4 pieces." for sites w/less than 4 blocks
  gather(Block, BlockString, BX1:BX4) %>% #2: re-condense into one column, generates correct number of rows per site AND per block
  mutate(Block = str_squish(BlockString), BlockString=NULL) %>% #3: replace placeholder column names w/"B1-B4". Also removes whitespace from BlockString contents introduced by "separate" function
  filter(!is.na(Block)) %>% #4: remove empty rows for sites w/out B3 or B4
  ### Unnest Spectra & calculate
  unnest(Spectra) %>% 
  filter(Wavelength > 400, Wavelength < 1000) %>% 
  mutate(CorrectionFactor = 1/Reflectance) %>% 

  ### The following code group repeated REF measurements, and takes the mean 
  group_by(Date,Site,Block,Wavelength) %>% 
  # group_by(Date,Site,Block,Wavelength, Integration_ms) %>% # to separate integration times
  summarize(ChA_REF = mean(ChA), ChB_REF = mean(ChB), CorrectionFactor = mean(ChA/ChB), int_REF = mean(Integration_ms), Notes_REF = str_c(Notes, collapse = "; "), ref_filenames = str_c(spu_filename,collapse = ", "))

## Join DATA with REFS

spu_for_plots <- df %>% filter(!str_detect(Treatment, "REF|DARK")) %>% 
  unnest(Spectra) %>% 
  filter(Wavelength > 400, Wavelength < 1000)


df_ref <- left_join(spu_for_plots, ref_summary) %>% 
  select(Date, DateTime, Site, Block, Treatment, Replicate, spu_filename, FileNum, Integration_ms, Weather, Notes, Notes_REF, ref_filenames, int_REF, Wavelength, ChB, ChA, ChB_REF, ChA_REF, CorrectionFactor) %>%
  mutate(raw_reflectance = ChB/ChA) %>% # the raw reflectance
  mutate(corrected_reflectance = raw_reflectance*CorrectionFactor) 


## Corrected Reflectances 
df_corrected <- df_ref %>% 
  nest(Wavelength:corrected_reflectance, .key = processed_spectra)
```

#### Check Application
```{r}
## Check all files have a corrected reflectance
corrected_spectra_files <- df_corrected %>% unnest(processed_spectra) %>% filter(!is.na(corrected_reflectance)) %>% select(spu_filename, Date, Site) %>% distinct()

anti_join(df_corrected, corrected_spectra_files) %>% print(n=50)

## Check that no important treatments are left out 
anti_join(df, df_corrected, by = "spu_filename") %>% pull(Treatment) %>% unique()
```

#### Plot Check
```{r}

df_subset <- df_corrected[100:110, ] %>% 
  unnest(processed_spectra) %>% 
  gather(key = Status, value = Reflectance, raw_reflectance, corrected_reflectance)
  
ggplot(data = df_subset, mapping = aes(x = Wavelength, y = Reflectance )) + 
  geom_line(aes(color = spu_filename, linetype = Status)) + 
    facet_wrap(vars(Date, Site, Block, Treatment, FileNum))
```

#### Format & Save
```{r}
processed_spu_data <- df_corrected %>% 
  select(spu_filename, DateTime, ref_filenames, processed_spectra) %>% unnest(processed_spectra) %>%
  select(spu_filename, DateTime, ref_filenames, Wavelength, ChB, ChA, raw_reflectance, CorrectionFactor, corrected_reflectance) %>% 
  nest(Wavelength:corrected_reflectance, .key = Spectra)

ref_spu_data <- ref_data %>% unnest(Spectra) %>% 
  select(spu_filename, DateTime, Wavelength, ChB, ChA, Reflectance) %>% 
  mutate(CorrectionFactor = 1/Reflectance, corrected_reflectance = NA) %>% 
  filter(Wavelength > 400, Wavelength < 1000) %>% 
  rename(raw_reflectance = Reflectance) %>% 
  nest(Wavelength:corrected_reflectance, .key = Spectra)

df_processed <- bind_rows(processed_spu_data, ref_spu_data)
  
## Save 
processed_spu_filename <- paste0("UnispecData/", dir_year, "_processed_spu_data.rds")

write_rds(df_processed, processed_spu_filename)
```


# QAQC
Quality check unispec data using the following code chunks. 

## Zoom Checks
### Plot Check
Run the following code chunk interactively in RStudio to check references at specific sites/dates. 
```{r check_refs}
## SPECIFY SITE/DATE/ETC to ZOOM IN ON
check_site <- "MAT"
check_date <- "2017-07-13" # necessary to unlist dates vector
check_treatments <- c("N")
check_blocks <- c("B1") 
first_file <- 20
last_file <- 27

data_check <- df %>% # full dataframe without maxed filtered 
  filter(Date == check_date) %>% 
  filter(Site %in% check_site) %>%
  # filter(Block %in% check_blocks) %>% 
  # filter(Treatment %in% check_treatments) %>% 
  #filter(Treatment == "REF") %>% 
  left_join(unispec_file_key) 

ref_check <- data_check %>% 
  unnest(Spectra) %>% 
  filter(FileNum >= first_file) %>%
  filter(FileNum <= last_file) %>%
  filter(Wavelength > 400, Wavelength < 1000) %>% 
  mutate(Reflectance = ChB/ChA) %>% 
  gather(key = Channel, value = Intensity, ChB, ChA) %>% 
  gather(key = ref_part, value = Reflectance_Intensity, Intensity, Reflectance)

## Plot Specified Correction Factors for quality check 
plot_zoom <- ggplot(data = ref_check, mapping = aes(x = Wavelength, y = Reflectance_Intensity)) +
  geom_line(aes(color=Treatment, linetype=Channel)) +
  facet_grid(ref_part ~ Date + Site + FileNum + Integration_ms, scales="free") 

plot_zoom

```

### Time Check
```{r time_check}

## SPECIFY SITE/DATE/ETC to ZOOM IN ON
check_site <- "LMAT"
check_date <- "2017-07-13" # necessary to unlist dates vector

## Files 
first_file <- 0
last_file <- 200

# Select columns to Check
timedata <- df %>% 
  filter(Date == check_date) %>% 
  filter(Site %in% check_site) %>% 
  filter(FileNum >= first_file) %>%
  filter(FileNum <= last_file) %>% 
  select(Site, Date, DateTime, FileNum, Integration_ms) %>% 
  group_by(DateTime) %>% distinct() 

timedata$diff <- timedata$DateTime - lag(timedata$DateTime)

meta_timedata <- left_join(timedata, unispec_file_key)

time_check <- meta_timedata %>% select(Site, Date, DateTime, FileNum, Block, Treatment, diff, Integration_ms) %>% ungroup()

# Examine dataframe
time_check %>% print(n=200)
```
Look at associated dataframe to check info, specifically: *Weather*, *Notes*, or *int* (Integration Time) columns.
```{r ref_check_files, echo=F}

## EXAMINE SPECIFIC info: check Integration, Notes, Weather, filename, etc. 
ref_check_files <- ref_check %>% 
  select(-c(Wavelength, ChA, ChB, CorrectionFactor_REF, keyname, Time)) %>% 
  unique()

### View dataframe of ref_check_files
kable(ref_check_files)
```

### Index Check

```{r}
index_check <- data_check %>% 
  mutate(NDVI = map(Spectra, function(x) calculate_indices(x, band_defns = band_defns, instrument = "MODIS", indices = "NDVI"))) %>% 
  unnest(NDVI) %>% 
  rename(NDVI = Value) %>% select(-Index)

index_check %>% select(Site, Block, Treatment, Replicate, FileNum, NDVI) 

ggplot(data = index_check, aes(x=FileNum,y=NDVI, fill=Treatment)) +
  geom_bar(stat="identity")
```


## Mis-Labeling 
Check for human error in recording correspondance between metadata and file numbers. 

### Discrepancy Checks
```{r df_check, dependson="df", eval= T, echo=F}
##### ------------------------- Variable checks 

# Date inconsistencies
spu_dataframe %>% mutate(Date_unispec = lubridate::date(DateTime)) %>% 
  filter(Date != Date_unispec)  %>% 
  select(Site, Date, Date_unispec) %>% distinct() %>%
  print(n=50)

# Missing spu files 
df %>% filter(!str_detect(Site, "_")) %>% 
  filter(is.na(spu_filename))  %>% 
  select(spu_filename, Site, FileNum, Date, Block, Treatment, Replicate, Notes, key_fix) %>% 
  print(n=100)

# Missing DateTime (equivalent to spu file)
df %>% filter(is.na(DateTime)) %>% 
  filter(!str_detect(Site, "_")) %>% 
  select_if(function(x) typeof(x) != "list") %>% summary()


# Block NA's (if in key)
df %>% filter(!is.na(Treatment)) %>% filter(is.na(Block)) %>% 
  filter(!str_detect(Treatment, "DARK|REF|VEG"))

df %>% filter(str_detect(Treatment, "REF"))  %>% 
  filter(is.na(Block)) %>% print(n=100)

# Replicate NA
df %>% filter(!is.na(Treatment)) %>% filter(is.na(Replicate)) %>% 
  filter(!str_detect(Treatment, "DARK|REF|VEG"))

## General NA test
df_na <- df %>% filter(is.na(spu_filename) |
              is.na(Site) |
              is.na(Block) & !str_detect(Treatment, "REF|DARK|VEG") | # Block NA's should always be REFS or EXTRA
              is.na(Replicate) & !str_detect(Treatment, "REF"), # Check for replicate NA's that aren't REF
              Treatment != "EXTRA|VEG|REF") # don't care about EXTRA
df_na %>% filter(Treatment !="REF") %>% select_if(function(x) typeof(x) != "list") %>% summary()

#####
## Check for large filenumbers : Inconsistent File Number reading due to number at end of site names
df_lgfn <- df %>%
  filter(FileNum > 500) %>%  # Sites w/numbers at the end mis-read in as part of file number. All > 100000: Unispec-DC measurements only go up to 5 digits.
  select(Date, Site, Block, Treatment, FileNum, spu_filename)
df_lgfn %>% print(n=50)


#####
## Check Site Names
df_sites <- unique(df$Site)
df_sites

## Check Treatment names
df_treatments <- unique(df$Treatment)
df_treatments

### -------------------------------------- SUMMARY CHECKS

## Dataframe 
df_summary <- df %>% group_by(Site, Date) %>%  
  filter(!is.na(Treatment)) %>% 
  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), Num_Files = n()) 

df_summary %>% print(n=100)

# Discrepancies  
## Site vs. Site_filename differences 
#### should only be Reference files or mislabeled (e.g. MAT extends into LMAT, or NANT continues to MNAT)
df_site_diff <- unispec_file_key %>% left_join(spu_key %>% select(-Date, -Type, -FileNum) %>% rename(Site_filename = Site)) %>%   filter(Site != Site_filename) 

df_site_diff %>% 
  group_by(Date) %>% 
  summarize(Sites = str_c(unique(Site), collapse = ", "), Sites_filename = str_c(unique(Site_filename), collapse = ", "), Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), Num_Files = n())

## Check File Number Pattern:
#### Num Files per block should be multiples of 5, unless REF or NA
df_filenum_count <- unispec_file_key %>% group_by(Site, Date, Block) %>% 
  filter(Treatment != "DARK") %>% 
  filter(Treatment != "REF") %>% 
  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), Num_Files = n()) %>% 
  #filter(Treatments != "NA|REF|DARK") %>% 
  filter(Num_Files %% 5 != 0)  # files per plot
  #filter(!str_detect(Treatments, "REF|NA")) 

df_filenum_count

df %>% inner_join(df_filenum_count) %>% print(n=100) 

```

Run the following code for Discrepancy check output. 
```{r, checks, eval = T, echo=F}

# cat(paste0("Dataframe Columns: ", str_c(df_names, collapse = ", ")))
# cat(paste0("Sites: ", str_c(df_sites, collapse = ", ")))
# cat(paste0("Treatments: ", str_c(df_treatments, collapse = ", ")))
# 
# df_na %>% kable(caption = "NA Metadata")
# df_lgfn %>% kable(caption = "Large FileNumbers")

df_names
df_sites
df_treatments

df_lgfn 
df_site_diff
df_filenum_count

df_summary

### Summarize  - look for NA's
unispec_file_key %>% select_if(function(x) typeof(x) != "list") %>% summary()
df %>% select_if(function(x) typeof(x) != "list") %>% summary()

```

### Duplicates 
Identify duplicates. Fix by editing unispec_key_fix.csv. Only duplicates should be REF files used for other sites. 

```{r duplicates}
### Check raw spu_files have no duplicates
nrow(spu_data) == length(unique(spu_data$spu_filename))

## Check for duplicates in key
unispec_file_key %>% group_by(Date, Site, Block, Treatment, Replicate, FileNum) %>%
  filter(n()>1) %>%
  arrange(Date, Site, FileNum) %>% 
  print(n=100)

## Check for duplicate in dataframe
duplicates <- df %>% 
  group_by(spu_filename) %>%
  filter(n()>1) %>%
  ungroup() %>%
  arrange(DateTime, spu_filename) %>% 
  select( spu_filename, FileNum, DateTime, Site, Block, Treatment, Replicate, Weather, everything())

# duplicates %>% print(n=200)
# duplicates %>% filter(!str_detect(Treatment, "REF")) %>% arrange(spu_filename) %>% print(n=100)

duplicates %>% 
  #mutate(Date = coalesce(Date, lubridate::date(DateTime))) %>% 
  group_by(Date) %>% 
  summarize(Sites = str_c(unique(Site), collapse = ","), Treatments = str_c(unique(Treatment), collapse = ","), Files  = n()) %>% 
  kable()

duplicates %>% filter(!str_detect(Treatment, "DARK|REF|VEG|TOWER"))

```


## Missing data 
List missing raw .spu files and those that are unprocessed (raw .spu file does not appear in summary .xlsx). 

### Missing raw files
```{r missing}
### MISSING: Find all processed data that is missing corresponding raw spu files 
missing_spu_data <- anti_join(unispec_file_key, spu_dataframe) %>% 
  filter(!str_detect(Site, "_"))

missing_spu_data %>% group_by(Date, Site) %>% 
  summarize(Treatments = str_c(unique(Treatment), collapse = ","), Files = n()) %>% 
  kable()

missing_spu_files <- missing_spu_data$spu_filename
unprocessed_spu_files <- anti_join(spu_dataframe, unispec_file_key)
```


## Mis-Measurement

### Max'd Out Spectra (> 65000 AD)
List files that maxed out, in the wavelengths used to calculate MODIS NDVI.
```{r maxed, dependson="df"}

maxed_data <- df %>% inner_join(spu_data %>% select(spu_filename)) %>% # restrict to those w/.spu files
  unnest(Spectra) %>% 
  filter(ChA > 65000 | ChB > 65000) %>% 
  group_by(spu_filename) %>% 
  summarize(maxed_number = n(), maxed_wavelengths = str_c(Wavelength, collapse = ", ")) 

maxed_files <- maxed_data$spu_filename

## Summary of Maxed files 
maxed_data %>% inner_join(df) %>% group_by(Site, Date) %>% 
  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), Num_Files = n(), 
            Num_Maxed = round(mean(maxed_number), digits = 1)) %>%
  kable()

## Select those Max'd in NDVI Region
maxed_limit <- 5 # keep files where spectra is max'd only at a narrow peak (e.g. for maxed_limit = 5, maxed region < 5*3.3nm = 16.5 nm)

maxed_data_bad <- df %>% inner_join(spu_data %>% select(spu_filename)) %>% unnest(Spectra) %>% 
  filter(ChA > 65000 | ChB > 65000) %>% 
  ## Extra conditions for NDVI region
  filter(Wavelength > 620) %>% # MODIS red lower bound
  filter(Wavelength < 876) %>% # MODIS nir upper bound
  ## Summary
  group_by(spu_filename) %>% 
  summarize(maxed_number = n(), maxed_wavelengths = str_c(Wavelength, collapse = ", ")) %>% 
  filter(maxed_number > maxed_limit)

## Summary of NDVI Max'd files 
maxed_data_bad %>% inner_join(df) %>% group_by(Site, Date) %>% 
  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), Num_Files = n(), 
            Num_Maxed = round(mean(maxed_number), digits = 1)) %>% 
  print(n=50)

maxed_files_bad <- maxed_data_bad$spu_filename
```


### Dim'd Out Spectra
Primarily Darkscans should show up. 
```{r dim, dependson=df}
dim_data <- df %>% inner_join(spu_data %>% select(spu_filename)) %>% unnest(Spectra) %>% 
  group_by(spu_filename) %>% 
  summarize(ChA_max = max(ChA)) %>% 
  filter(ChA_max < 20000) %>% 
  left_join(df) 

# Summary of important info
dim_data %>% group_by(Date, Site) %>% 
  filter(!is.na(Treatment)) %>% 
  filter(!str_detect(Treatment, "DARK|Throwawayscan")) %>% 
  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), 
            Types = str_c(str_replace_na(unique(Type)), collapse = ","), 
            Files = n(), ChA_Max = max(ChA_max))  %>% 
  kable()

# File List
dim_files <- dim_data$spu_filename
```


### Absurd Reflectance > 1

#### Zero'd Spectra
```{r zerod}
## zero'd data 
zero_data <- df %>% inner_join(spu_data %>% select(spu_filename)) %>% unnest(Spectra) %>% 
  filter(ChA == 0) %>%  ## This is for all wavelengths, not just 400-1000nm
  group_by(spu_filename) %>% 
  summarize(Zeros = n()) %>% 
  left_join(df)
  
## Summary 
zero_data %>% group_by(Date, Site) %>% 
  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), 
            Files = n(),
            Max_Zeros = max(Zeros)) %>% 
  kable()


## File List : Restricted Wavelengths
zero_files <- zero_data %>% unnest(Spectra) %>% 
  filter(ChA == 0 ) %>% 
  filter(Wavelength > 400, Wavelength < 1000) %>% 
  pull(spu_filename) %>% unique()

zero_data_narrowed <- df %>% filter(spu_filename %in% zero_files) 

## Plot Exploration
zero_data_narrowed %>% 
  slice(1:5) %>% plot_channels()

## Summary
zero_data_narrowed %>% group_by(Date, Site) %>% 
  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), Files = n())  %>% 
  kable()
  
```

#### Reflectance >1 
```{r absurd}

## Raw Spectra
absurd_raw_data <- df %>% 
  filter(! Spectra %>% map(is.null) %>% map_lgl(any)) %>% # remove files w/out spu_spectra
  filter(!str_detect(Treatment, "REF")) %>% # ignore REF files
  unnest(Spectra) %>% # use raw spectra
  filter(Wavelength > 400 & Wavelength < 1000) %>% 
  filter(Reflectance > 1) %>% 
  nest(Wavelength, ChA, ChB, Reflectance, .key = Spectra)

absurd_raw_files <- absurd_raw_data$spu_filename

absurd_raw_data %>% group_by(Date, Site) %>% 
  summarize(Treatments = str_c(unique(Treatment), collapse = ","), Files = n())

## Corrected Spectra
absurd_data <- df_corrected %>% 
  filter(!str_detect(Treatment, "REF")) %>% # ignore REF files
  # filter(!spu_filename %in% unprocessed_spu_files) %>% # leave out missing multispec data
  unnest(processed_spectra) %>% # use corrected spectra
  select(-(ChB:raw_reflectance)) %>%  # remove unnecessary Wavelength specific rows 
  filter(Wavelength > 400 & Wavelength < 1000) %>% 
  filter(corrected_reflectance > 1) %>% 
  nest(Wavelength, corrected_reflectance, .key = corrected_spectra)

## Summary
absurd_data %>% group_by(Date, Site) %>% 
  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), Files = n())  %>% 
  kable()

## File List 
absurd_files <- absurd_data$spu_filename

## Plot Check 
absurd_data %>% 
  inner_join(spu_data) %>% 
  slice(1:10) %>% 
  plot_channels()

```



# Label Unispec Problems 

## Problem Key 
```{r unispec_problem_key}

df_ref_filenames <- df_corrected %>% unnest(processed_spectra) %>% 
  select(spu_filename, ref_filenames) %>% distinct()

unispec_problem_key_pre <- unispec_file_key %>% 
  mutate(Replicate = factor(Replicate)) %>% 
  # mislabeled 
  mutate(mislabeled = !is.na(key_fix)) %>% # mislabeled 
  # mismeasurement
  left_join(maxed_data) %>% # maxed_number, maxed_wavelenghs
  mutate(maxed = maxed_number > 5) %>% # choose how strict to make this using maxed_number threshold, etc. # mutate(maxed = spu_filename %in% maxed_data_files_bad) %>% 
  mutate(dim = spu_filename %in% dim_files) %>% # dim
  mutate(absurd_reflectance = spu_filename %in% c(zero_files, absurd_files)) %>% # absurd_reflectance
  # missing 
  mutate(missing_spu = spu_filename %in% missing_spu_files) %>%  # missing_spu
  left_join(df_ref_filenames)
  
# miscorrection 
ref_problem_key <- unispec_problem_key_pre %>%
  filter(spu_filename %in% ref_files) %>% # select ref files 
  #mutate(mislabeled = if_else(spu_filename %in% ref_mistakes, TRUE, mislabeled)) %>% 
  gather("problem", "status", mislabeled, maxed, dim, absurd_reflectance, missing_spu) %>% # row for each type of problem
  filter(!is.na(status)) %>% # remove non-problems
  filter(status != FALSE) %>% # remove non-problems
  group_by(spu_filename) %>% 
  select(spu_filename, problem, status) %>% 
  summarize(problems = str_c(unique(problem), collapse = ", ")) %>% # group: one row per spu_filename
  rename(ref_filenames = spu_filename, ref_problem = problems) # rename variables to join w/unispec_problem_key

## Unispec Problem Key
unispec_problem_key <- unispec_problem_key_pre %>% 
  separate_rows(ref_filenames, sep = ", ") %>% # split ref_filenames string into multiple rows per ref_filename
  left_join(ref_problem_key) %>% # adds ref_problem column 
  group_by(spu_filename) %>% 
  summarize(ref_problems = str_c(ref_problem, collapse = "; ")) %>% # collapse to one row per spu_filename
  right_join(unispec_problem_key_pre) %>%  # add the rest of the key info back in
  mutate_at(.vars = c("ref_problems", "dim", "absurd_reflectance", "mislabeled", "missing_spu"), .funs = factor) %>% 
  mutate(file_problem = any(maxed, dim, absurd_reflectance)) %>% 
  select(spu_filename, Type, Date, Site, Block, Treatment, Replicate, Weather, Notes, missing_spu, file_problem, mislabeled, key_fix, maxed, maxed_number, maxed_wavelengths, dim, absurd_reflectance, ref_problems, ref_filenames) # order

### Summary table
unispec_problem_key %>%
  mutate_at(funs(factor), .vars = vars(c("Site", "Block", "Treatment", "key_fix", "ref_problems"))) %>% 
  summary() 
```

### Save Unispec Problem Key
All the information you could ever want in one .csv! 
```{r}
## Save .csv of file key
unispec_problem_keyname <- paste0("UnispecData/", dir_year,"_unispec_file_key_problems.csv")
write_csv(unispec_problem_key, path = unispec_problem_keyname)
```

# Check Spectral Data

## Plot Spectra: Site vs. Block
For the following code, make sure you select only one date per site. The 5 measurements per plot are averaged as a line and one standard deviation above and below shaded. Interactively edit the "PLOT SELECTION" vectors in the code chunk below to investigate specific data.

```{r plot_spectra_blocks, echo=F}

df_tidy <- unispec_problem_key %>% filter(!is.na(Treatment)) %>% 
  left_join(df_processed) %>% 
  mutate(Date = lubridate::date(Date))
  
# Recall df_table for list of dates/sites
kable(df_table) 

#PLOT SUBSET -- SPECIFY SITE/DATE/ETC to ZOOM IN ON
check_dates <- c("2017-07-27", "2017-06-27")
check_sites <- c("MAT", "LMAT")
check_blocks <- c("B1", "B2", "B3", "B4")
check_trtmts <- c("NP", CT) 

# Data Comparison Format
df_block <- df_tidy %>% 
  filter(format(DateTime, format="%Y-%m-%d") %in% check_dates) %>% 
  filter(Site %in% check_sites) %>% 
  filter(Block %in% check_blocks) %>% 
  filter(Treatment %in% check_trtmts) %>% 
  unnest(Spectra) %>% 
  rename(Reflectance = corrected_reflectance) %>% 
  group_by(Site, Block, Treatment, Date, Wavelength) %>% 
  summarize(
    avg_reflect = mean(Reflectance),
    max_ref = max(Reflectance),
    min_ref = min(Reflectance),
    sd_reflect = sd(Reflectance)
    ) 

ggplot(data = df_block, mapping = aes(x = Wavelength, y = avg_reflect)) +
  geom_line(aes(color=Treatment)) + 
  geom_ribbon(aes(ymin=avg_reflect-sd_reflect, ymax=avg_reflect+sd_reflect, fill=Treatment), alpha=0.25) +
  facet_grid(Date ~ Site + Block ) + 
  scale_color_manual(values=c("CT" = "forestgreen", "CT1"="darkolivegreen2", "CT2"="darkolivegreen3",
                              "N" = "dodgerblue", "NO3" = "skyblue", "NH4" = "deepskyblue",
                              "P" = "red2",
                              "NP" = pur_pal[5],
                              "F0.5" = pur_pal[1],
                              "F1" = pur_pal[2],
                              "F2" = pur_pal[3],
                              "F5" = pur_pal[4],
                              "F10" = pur_pal[5])) + 
  scale_fill_manual(values=c("CT" = "forestgreen", "CT1"="darkolivegreen2", "CT2"="darkolivegreen3",
                              "N" = "dodgerblue", "NO3" = "skyblue", "NH4" = "deepskyblue",
                              "P" = "red2",
                              "NP" = pur_pal[5],
                              "F0.5" = pur_pal[1],
                              "F1" = pur_pal[2],
                              "F2" = pur_pal[3],
                              "F5" = pur_pal[4],
                              "F10" = pur_pal[5]))

```


 
## Plot Spectra: Site vs. Date
Average plot averages by block. NOTE: Ask Laura what the correct error propogation is here.

```{r plot_spectra_dates, echo=F}
#PLOT SUBSET -- SPECIFY SITE/DATE/ETC to ZOOM IN ON
check_dates <- ymd("2017-07-27", "2017-06-28")
check_sites <- c("MAT", "LMAT")
check_blocks <- c("B1", "B2", "B3", "B4")
check_trtmts <- c("NP", CT) 

# Data Comparison Format
df_dates <- df_tidy %>% 
  filter(format(Date, format="%Y-%m-%d") %in% check_dates) %>% 
  filter(Site %in% check_sites) %>% 
  filter(Block %in% check_blocks) %>% 
  filter(Treatment %in% check_trtmts) %>%
  group_by(Site, Block, Treatment, Date, Wavelength) %>% # average measurements by plot
  summarize(
    avg_reflect = mean(Reflectance),
    max_ref = max(Reflectance),
    min_ref = min(Reflectance),
    sd_reflect = sd(Reflectance)
    )  %>% 
  group_by(Site, Treatment, Date, Wavelength) %>% # average plots by block
  summarize(
    block_avg_reflect = mean(avg_reflect),
    max_ref = max(avg_reflect),
    min_ref = min(avg_reflect),
    sd_reflect = sd(avg_reflect)
    ) 

ggplot(data = df_dates, mapping = aes(x = Wavelength, y = block_avg_reflect)) +
  geom_line(aes(color=Treatment)) + 
  geom_ribbon(aes(ymin=block_avg_reflect-sd_reflect, ymax=block_avg_reflect+sd_reflect, fill=Treatment), alpha=0.25) +
  facet_grid(Site ~ Date) + 
  scale_color_manual(values=c("CT" = "forestgreen", "CT1"="darkolivegreen2", "CT2"="darkolivegreen3",
                              "N" = "dodgerblue", "NO3" = "skyblue", "NH4" = "deepskyblue",
                              "P" = "red2",
                              "NP" = pur_pal[5],
                              "F0.5" = pur_pal[1],
                              "F1" = pur_pal[2],
                              "F2" = pur_pal[3],
                              "F5" = pur_pal[4],
                              "F10" = pur_pal[5])) + 
  scale_fill_manual(values=c("CT" = "forestgreen", "CT1"="darkolivegreen2", "CT2"="darkolivegreen3",
                              "N" = "dodgerblue", "NO3" = "skyblue", "NH4" = "deepskyblue",
                              "P" = "red2",
                              "NP" = pur_pal[5],
                              "F0.5" = pur_pal[1],
                              "F1" = pur_pal[2],
                              "F2" = pur_pal[3],
                              "F5" = pur_pal[4],
                              "F10" = pur_pal[5]))

```


# Check Vegetation Indices

Currently, this only works for NDVI, EVI, and EVI2 as I haven't worked out spectral interpolation yet and the other indices need reflectance at a specific value (not a range). 

## Calculate Indices
```{r}
df_indices <- unispec_file_key %>% 
  filter(!is.na(Treatment)) %>% 
  filter(!str_detect(Treatment, "REF|DARK")) %>% 
  left_join(df_processed) %>% 
  ## Format for calculuating indices
  unnest(Spectra) %>% 
  select(-ChB, -ChA, -raw_reflectance, -CorrectionFactor) %>% 
  rename(Reflectance = corrected_reflectance) %>% 
  nest(Wavelength, Reflectance,.key = Spectra) %>%
  ## Calculate NDVI
  mutate(Indices = map(Spectra, function(x) calculate_indices(x, band_defns = band_defns, instrument = "MODIS", indices = c("NDVI", "EVI", "EVI2"))))
  
index_filename <- paste0("UnispecData/", dir_year, "_index_data.rds")
write_rds(df_indices, index_filename)
  
```

## Plot Check NDVI
```{r}

## SPECIFY SITE/DATE/ETC to ZOOM IN ON
check_site <- "MAT"
check_date <- "2017-07-27" # necessary to unlist dates vector
check_treatments <- c("NP")
check_blocks <- c("B1", "B2", "B3", "B4") 
first_file <- 0
last_file <- 100

df_subset <- df_indices %>% 
  filter(Date == check_date) %>% 
  filter(Site %in% check_site) %>%
  filter(Block %in% check_blocks) %>% 
  filter(Treatment %in% check_treatments)
  
  
index_subset <- df_subset %>% 
  unnest(Indices) %>% 
  filter(Index == "NDVI")

spec_subset <- df_subset %>% select(-Spectra) %>% 
  filter(Date == check_date) %>%
  left_join(spu_data) 

spec_subset %>% plot_channels() 
  
ggplot(data = index_subset,aes(x=Date, y=Value)) +
  geom_point(aes(color=Treatment, shape = Block)) 

ggplot(data = index_subset, aes(x=FileNum,y=Value, fill=Block)) +
  geom_bar(stat="identity") + facet_wrap(vars(Block), scales = "free_x")

ggplot(data = index_subset, aes(x=Date,y=Value)) +
  geom_point(aes(color=Block, shape=factor(Replicate)))
  #geom_line(aes(group=Replicate, color=Block))
  
```


# PLOTS for LTER Site Review
## Plot Options 
```{r ndvi, echo=F}

## PLOT OPTIONS
# Color Palettes 
n_yellow <- rgb(255, 192, 0, maxColorValue = 255)
p_blue <- rgb(46, 117, 182, maxColorValue = 255)
np_green <- rgb(112, 173, 71, maxColorValue = 255) #lmat_colors[5] 
ct_gray <- rgb(175, 171, 171, maxColorValue = 255)

lmat_colors <- c(rgb(226, 240, 217, maxColorValue = 255), rgb(169, 209, 142, maxColorValue = 255), rgb(112, 173, 71, maxColorValue = 255), rgb(84, 130, 53, maxColorValue = 255), rgb(56, 87, 35, maxColorValue = 255))
  
  # colorRampPalette(c(rgb(175, 171, 171, maxColorValue = 255), rgb(112, 173, 71, maxColorValue = 255)), alpha = TRUE, bias=3)(6)

 #RColorBrewer::brewer.pal(5, "Greens") 

## TEXT
Site.text <- c("MAT (started 1989)", "LMAT (started 2006)") 
names(Site.text) <- c("MAT", "LMAT")

## EXCLUSION 
bad_files <- c("2017-07-13_MAT_00025.spu")
```

## NDVI SUMMER plot
```{r}
#PLOT SUBSET -- SPECIFY SITE/DATE/ETC to ZOOM IN ON
check_dates <- df_table %>% select(Date) %>% pull()#ymd("2017-06-08", "2017-06-15", "2017-06-26")
check_sites <- c("MAT")
check_blocks <- c("B1", "B2", "B3", "B4")
check_trtmts <- c(CT, "F0.5", "F1", "F2", "F5", "F10", "N", "P", "NP")  

ndvi_subset <- df_indices %>% select(spu_filename, Spectra, Indices) %>% 
  left_join(unispec_file_key) %>% 
  mutate(Date = lubridate::date(Date)) %>% 
  filter(Site %in% check_sites) %>% 
  filter(Block %in% check_blocks) %>% 
  filter(Treatment %in% check_trtmts) %>%
  unnest(Indices) %>%
  filter(Index == "NDVI") %>% 
  select(-Index) %>% rename(NDVI = Value) %>% 
  filter(!spu_filename %in% bad_files)

ndvi_plot <- ndvi_subset %>% 
  mutate(Treatment = replace(Treatment, Treatment %in% CT, "CT")) %>% 
  mutate(Treatment = factor(Treatment, levels = check_trtmts)) %>% 
  group_by(Site, Block, Treatment, Date) %>% 
  summarise(
    avg_ndvi = mean(NDVI),
    sd_ndvi = sd(NDVI)
  ) %>% 
  group_by(Site,Treatment,Date) %>% 
  summarise_at(.vars = vars(avg_ndvi), .funs = funs(mean,sd,se=sd(.)/sqrt(n())))
  

ndvi_ggplot <- ggplot(data = ndvi_plot, mapping = aes(x = Date, y = mean, color=Treatment)) +
  geom_line(size=1.5) + 
  geom_errorbar(size = 1.25, aes(ymin = mean-se, ymax= mean + se), width=2) + 
  geom_point(size=3) +
  facet_grid(. ~ Site, labeller = labeller(Site = Site.text)) + 
  scale_color_manual(values=c("CT" = ct_gray, "CT1" = ct_gray, "CT2"= ct_gray,
                              "N" = n_yellow, "NO3" = "skyblue", "NH4" = "deepskyblue",
                              "P" = p_blue,
                              "NP" = lmat_colors[5],
                              "F0.5" = lmat_colors[1],
                              "F1" = lmat_colors[2],
                              "F2" = lmat_colors[3],
                              "F5" = lmat_colors[4],
                              "F10" = lmat_colors[5])) 

figure <- ndvi_ggplot +
  labs(title ="",
       x = "",
       y = "NDVI") + 
  ylim(c(0.33, 0.87)) + 
  theme_bw(base_size=18) +
  theme(legend.justification = c(1.15,-0.13), legend.position = c(1,0), legend.background = element_rect(color = "grey", linetype = "solid", size = 0.8), 
        ## Facet text 
        strip.text.x = element_text(
        size = 18, face = "bold"
        ))

figure

ggsave(paste0("2017_", check_sites, "_NDVI.png"), width = 6)

```

## LTER Site Review Plot
```{r}

grid.arrange(figure, figure_lmat, ncol = 2, top=textGrob("Daily QC: Blue",gp=gpar(fontsize=20,font=3))
)

ggsave("LTER_2017_NDVI_figure.png", width = 6)


```


