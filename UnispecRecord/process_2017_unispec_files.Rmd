---
title: "Process LTER Unispec Data"
author: "Ruby An"
date: "December 17, 2018"
output:
  html_document:
    df_print: paged
  html_notebook:
    number_sections: yes
editor_options:
  chunk_output_type: console
---

# How to use this R Markdown Notebook: 

This R Markdown document walks through processing historic (already collected) unispec data from 2010-2018. 

# Required Packages
```{r setup, echo=F}
knitr::opts_chunk$set(echo = TRUE)

## Required Packages
library("tidyverse")
library("knitr")
source("unispec_functions.R") # file loads required functions

## Useful vectors for standardizing names and filtering data
WSG <- c("WSG1", "WSG23")
SHB <- c("SHB1", "SHB2")
site_list <- list("MAT", "LMAT", "MNAT", "NANT", "DHT", WSG, SHB, "HST")

CT <- c("CT","CT1","CT2")
NP_gradient <- c("F0.5","F1","F2","F5","F10sz-
                 -zeoe.") # for LOF site
N_types <- c("NO3", "NH4") # for LOF site
trtmt_list <- list(CT, "N", "P", "NP", NP_gradient, N_types)

## Useful vectors for plotting
# Color sequences
pur_pal <- RColorBrewer::brewer.pal(5, "Purples")
```


# Choose Directory

Select the folder containing the unispec files you want to process. Run the following code chunk interactively in RStudio to set folder via pop-up window. 
```{r directory}
interact <- interactive() 

if (interact) { 
  ## INTERACTIVE CODE (use when in RStudio)
  library("rChoiceDialogs") # for interactively selecting file directories
data_path <- rchoose.dir(caption = "Select Unispec files directory")
} else { 
  ## STATIC CODE (use when Knitting)
  data_path  <- "UnispecData/2017" 
}
```

**Chosen Directory**: `r data_path`

This directory should contain both the `.spu` files you wish to process and a corresponding `*_unispec_key.csv` file. The key file matches the .spu files to the date, site, block, treatment, plot, & measurement and specifies which white references to use to correct for instrument error. 

# Load Unispec Data 
Run the following code chunks to load & join keys to data from your chosen directory. 

## Load Keys 
Select unispec key files interactively or search within: `r data_path`. 

```{r key} 
if (interact) {
  key_files <- rchoose.files() # choose via window
} else {
  ## Find all file keys matching search pattern 
  key_files <- list.files(path = data_path, pattern = "*_key.csv", full.names = T, recursive = T)
} 

## Read in data from filekeys 
key_list <- tibble(keyname = key_files) %>% # create dataframe
  mutate(key_contents = map(keyname, function(x) read_key_file(x))) 
# read_key_file() is a function I wrote located in the file "unispec_functions.R"
# map function: super useful to apply function to objects without slow "for" loops

## Unpack into usable dataframe 
keys <- unnest(key_list)
```
**Chosen Keys**: `r key_files`

## Load Data
You can choose to select unispec data files interactively; however, it is usally more convenient to let R search: `r data_path` -- for all `.spu` files. This step may take several minutes, if you chose a folder containing many files.


```{r spu_data, cache =T}

if (interact) {
  files <- rchoose.files(filter=c("*.spu")) # choose via window
} else {
  ## Find .spu files via search pattern: specify Date(s) or Site(s) to read files
  files <- list.files(path = data_path, pattern = ".spu$", full.names = T, recursive=T)
} 

## Read data from files (can take minutes)
data_list <- tibble(filename = files) %>% # create dataframe
  mutate(file_contents = map(filename, function(x) read_spu_file(x)))

## Unpack into usable dataframe 
data <- unnest(data_list) %>% 
  filter(Wavelength >= 400 & Wavelength <= 1000) # trim unreliable wavelengths
```


The following table lists the dates and sites of the files contained in your chosen directory: 

## Join Data & Keys
```{r join_keys_data}
## Join all data matching keys dataframe (drops any non-matching data),
##  by columns (Date, Site, FileNum)
df <- inner_join(data, keys)  

## Sites per Date table to display
df_table <- df %>% 
  select(Date, Site) %>% 
  distinct() %>% 
  group_by(Date) %>% 
  do(Sites = t(.[-1])) %>% 
  mutate(Sites = paste( unlist(Sites), collapse=','))

kable(df_table)
```



# QAQC
Quality check unispec data using the following code chunks. 

## Max'd Out Spectra (> 65000 AD)
List files that maxed out. 
```{r maxed_pectra}
maxed_data_files <- df %>% filter(ChA > 65000 | ChB > 65000) %>% # list of files w/max'd spectra
  group_by(filename, Date, Site, FileNum, Block, Treatment, Measurement, int, Weather, Notes) %>% 
  summarize(n_maxed = n()) %>% 
  filter(n_maxed > 1) %>%  
  ungroup() # necessary in order to slice rows 

maxed_limit <- 5 # keep files where spectra is max'd only at a narrow peak (e.g. for maxed_limit = 5, maxed region < 5*3.3nm = 16.5 nm)

bad_maxed_data_files <- maxed_data_files %>% 
  filter(n_maxed > maxed_limit) %>% 
  ungroup() %>%  # necessary inorder to slice rows 
  arrange(Date, Site, Block, Treatment, Measurement)

kable(bad_maxed_data_files)

```

Plot a subset of maxed out spectra. 

```{r maxed_spectra_plot}
## Plot max'd spectra 
first_file <- 61 # choose subset of files to plot 
last_file <- 71 # > 20 is really slow / scrunched

maxed_spectra <- bad_maxed_data_files %>% 
  slice(first_file:last_file) %>% # don't plot too many 
  left_join(df, by = c("filename", "Date", "Site", "Block", "Treatment", "Measurement", "FileNum", "int")) %>% # all wavelengths 
  gather(key = Channel, value = Intensity, ChB, ChA) 

maxed_spectra_plot <- ggplot(maxed_spectra, aes(x=Wavelength, y= Intensity)) + 
  geom_line(aes(color=Channel)) + 
  geom_hline(yintercept = 65000, color="black",linetype=3) +
  facet_wrap(vars(Date,Site,Block,Treatment,Measurement), labeller = label_wrap_gen(multi_line=FALSE))

maxed_spectra_plot
```


### Remove Max'd Out Spectra
Removes maxed spectra from dataframe, and creates file listing removed spectra. 
```{r remove_maxed_pectra, echo=F}
## Remove max'd data from df
df_clean <- anti_join(df, bad_maxed_data_files)

## Save CSV of max'd spectra filenames 
#write_csv(maxed_data_files, "UnispecData/2017/2017_maxed_spu_files.csv")

```

## White References
White references correct for instrument & cable irregularities. Multiplying by the correction factor (ChA/ChB) smooths out the spectra. If multiple file numbers are listed (typically 5), the correction factors are averaged. 

### List References
The following code chunk plots all the white reference spectra in your chosen directory.

```{r refs, echo = F, warning=F}

## Find all white reference data files 
ref_data_all <- df_clean %>% 
  filter(str_detect(Treatment, "REF")) %>% # extract reference data 
  ## The following steps expand the "Block" column tocreate one REF set per Block per Site -- as in the keys above.
  separate(Block, into = c("BX1", "BX2", "BX3", "BX4"), sep = ",", fill="right") %>% #1
  gather(Block, BlockString, BX1:BX4) %>% #2
  mutate(Block = str_squish(BlockString), BlockString=NULL) %>% #3
  filter(!is.na(Block)) %>% #4
  mutate(REF_CorrectionFactor = ChA/ChB) %>%  # calculate correction fact column
  arrange(Date, Site, FileNum)

ref_table <- ref_data_all %>% 
  select(Date, Site) %>% 
  distinct() %>% 
  group_by(Date) %>% 
  do(Sites = t(.[-1])) %>% 
  mutate(Sites = paste( unlist(Sites), collapse=','))

kable(ref_table)

ref_files <- ref_data_all %>% 
  select(filename, Date, Site, FileNum, Treatment, Measurement) %>% 
  unique()
```

### Plot References
```{r ref_plot}
## Plot a subset of reference spectra
first_file <- 1 # choose subset of files to plot 
last_file <- 10 # avoid plotting >50 at a time

## Select a subset to plot 
ref_subset <- ref_files %>% 
  slice(first_file:last_file) %>% 
  left_join(ref_data_all, by = c("filename", "Date", "Site", "Treatment", "Measurement", "FileNum")) %>% 
  gather(key = Channel, value = Intensity, ChB, ChA) %>% 
  gather(key = ref_part, value = CorrectionFactor_Intensity, Intensity, REF_CorrectionFactor)
  
ref_plot <- ggplot(data = ref_subset, mapping = aes(x = Wavelength, y = CorrectionFactor_Intensity)) +
  geom_line(aes(color=Measurement, linetype=Channel)) +
  facet_grid(ref_part ~ Date + Site + int, scales="free") 

ref_plot

```
Look for correction factors very far from 1.0 or with odd peaks. Edit the key to resolve errors as you go. 

## Plot Check
Run the following code chunk interactively in RStudio to check references at specific sites/dates. 
```{r check_refs}
## SPECIFY SITE/DATE/ETC to ZOOM IN ON
check_site <- "MNAT"
check_date <- "2017-08-05" # necessary to unlist dates vector
first_file <- 0
last_file <- 12

ref_check <- data %>% # full dataframe without maxed filtered 
  filter(Date == check_date) %>% 
  filter(Site %in% check_site) %>% 
  #filter(Treatment == "REF") %>% 
  filter(FileNum >= first_file) %>%
  filter(FileNum <= last_file) %>%
  mutate(Reflectance = ChB/ChA) %>% 
  gather(key = Channel, value = Intensity, ChB, ChA) %>% 
  gather(key = ref_part, value = Reflectance_Intensity, Intensity, Reflectance)

## Plot Specified Correction Factors for quality check 
plot_zoom <- ggplot(data = ref_check, mapping = aes(x = Wavelength, y = Reflectance_Intensity)) +
  geom_line(aes(color=int, linetype=Channel)) +
  facet_grid(ref_part ~ Date + Site + FileNum + int, scales="free") 

plot_zoom
```

## Time Check
```{r time_check}
first_file <- 0
last_file <- 200

# Select columns to Check
timedata <- data %>% 
  filter(Date == check_date) %>% 
  filter(Site %in% check_site) %>% 
  filter(FileNum >= first_file) %>%
  filter(FileNum <= last_file) %>% 
  select(Site, Date, FileNum, Time, int) %>% 
  group_by(Time) %>% distinct() 

timedata$diff <- timedata$Time - lag(timedata$Time)
time_check <- timedata %>% select(Site, Time, FileNum, diff, int) %>% ungroup

# Examine dataframe
time_check
```
Look at associated dataframe to check info, specifically: *Weather*, *Notes*, or *int* (Integration Time) columns.
```{r ref_check_files, echo=F}

## EXAMINE SPECIFIC info: check Integration, Notes, Weather, filename, etc. 
ref_check_files <- ref_check %>% 
  select(-c(Wavelength, ChA, ChB, CorrectionFactor_REF, keyname, Time)) %>% 
  unique()

### View dataframe of ref_check_files
kable(ref_check_files)
```

## Apply White References:
Based on the above quality check, choose reference files by entering the appropriate file numbers in **`r key_files`** for the rows where the column *Treatment* = **REF**. There are typically 5 reference measurements per *Date* / *Site*. 

Then rerun the **Load Keys**. The following plots your chosen references.

```{r ref_table, echo=F}
options(knitr.kable.NA = '')

## Build REF key 
ref_keys <- keys %>% 
  filter(Treatment == "REF") %>% # extract reference data 
  ## The following separates the Site column into "Site" and "Site_REF"
  ### Site = the site to which the reference measurements should be applied 
  ### Site_REF = the site where the reference measurements were actually collected
  separate(Site, into=c("Site", "Site_REF"), sep = "_", fill="right") %>% 
  mutate(Site_REF = coalesce(Site_REF, Site)) # if the references were collected at 'Site', the created column Site_REF will be NA. Use coalesce() to fill these NA's with the value of "Site".  


## Find REF files for correction factors
ref_data <- data %>% # in the dataframe "data", the "Site" column is the location where the data was collected 
  rename(Site_REF = Site) %>% # we thus rename to match 'ref_keys'
  inner_join(ref_keys) %>% # data & ref_keys are joined by = c("Site_REF", "FileNum", "Date") 
  ## "Site_REF" is the location where the file (from which the reference correction factor is calculated) actually was collected
  ## "Site", inherited from ref_keys, is now the location where the correction factor should be applied
  mutate(CorrectionFactor_REF = ChA/ChB) 

```

### List Chosen References
```{r ref_choice_list}
# ## Plot CHOSEN Correction Factors -- to look at spread of reference data
# cor_factor_plot  <- ggplot(data = ref_data, mapping = aes(x = Wavelength, y = CorrectionFactor_REF)) +
#   geom_line(aes(color=Measurement, linetype=factor(int))) +
#   facet_wrap(vars(Date,Site_REF, Block), ncol=4)


## Output Table of Chosen References
ref_choice <- ref_data %>% 
  select(Date, Site, Site_REF, FileNum, Block, Measurement, Weather, Notes, filename) %>% 
  distinct() %>% 
  mutate(Measurement = str_c("P", Measurement))%>% 
  spread(Measurement, FileNum) %>% 
  unite(FileNums, P1:P5, sep=",") 

kable(ref_choice, caption="White Reference Files")

```

### Correct with References
Rerun the **Join Data & keys** sections above to update the `df_clean` dataframe.Apply your chosen references to actual spectral data to create the tidy dataframe `df_tidy` containing corrected sepectral reflectance values.

```{r apply_refs, echo=FALSE}
## Average 5 chosen ref measurements per DATE/SITE/BLOCK 
ref_summary <- ref_data %>% 
  ## The following steps expand the "Block" column to create one REF set per Block per Site. This structure is necessary for situtations where different refs are used for different blocks at the same site. 
  separate(Block, into = c("BX1", "BX2", "BX3", "BX4"), sep = ",") %>% #1: expand string entry in "Block" into separate columns
  gather(Block, BlockString, BX1:BX4) %>% #2: re-condense into one column, generates correct number of rows per site AND per block
  mutate(Block = str_squish(BlockString), BlockString=NULL) %>% #3: replace placeholder column names w/"B1-B4". Also removes whitespace from BlockString contents introduced by "separate" function
  filter(!is.na(Block)) %>% #4: remove empty rows for sites w/out B3 or B4
  ### The following code group repeated REF measurements, and takes the mean 
  group_by(Date,Site,Block,Wavelength) %>% 
  # group_by(Date,Site,Block,Wavelength, int) %>% # to separate integration times
  summarize(ChA_REF = mean(ChA), ChB_REF = mean(ChB), CorrectionFactor_REF = mean(ChA/ChB), int_REF = mean(int), Notes_REF = str_c(Notes, collapse = "; "), FileNames_REF = str_c(filename,collapse = ", "))

## Join DATA with REFS 
df_ref <- inner_join(df_clean, ref_summary) %>% 
  select(Date, Time, Site, Block, Treatment, Measurement, Wavelength, int, int_REF, ChB, ChA, ChB_REF, ChA_REF, CorrectionFactor_REF, Weather, Notes, Notes_REF, FileNames_REF, filename, FileNum, keyname) %>%
  mutate(raw = ChB/ChA) %>% # the raw reflectance
  mutate(correct = raw*CorrectionFactor_REF) %>% # this step calculates the corrected reflectance
  gather(Type, Reflectance, raw:correct) 

df_tidy <- df_ref %>% 
  mutate(Site = replace(Site, Site %in% WSG, "WSG")) %>% # rename WSG1 & WSG23 to WSG
  mutate(Site = replace(Site, Site %in% SHB, "SHB"))# %>%  # rename  SHB1 & SHB2 to SHB
  # filter(Type == "correct") %>% # drop raw reflectance
  # select(-Type) 
```

# Check Spectral Data

## Plot Spectra: Site vs. Block
For the following code, make sure you select only one date per site. The 5 measurements per plot are averaged as a line and one standard deviation above and below shaded. Interactively edit the "PLOT SELECTION" vectors in the code chunk below to investigate specific data.



```{r plot_spectra_blocks, echo=F}
# Recall df_table for list of dates/sites
kable(df_table) 

#PLOT SUBSET -- SPECIFY SITE/DATE/ETC to ZOOM IN ON
check_dates <- ymd("2017-07-21", "2017-06-15")
check_sites <- c("MAT", "LMAT")
check_blocks <- c("B1", "B2", "B3", "B4")
check_trtmts <- c("NP", CT) 

# Data Comparison Format
df_block <- df_tidy %>% 
  filter(Date %in% check_dates) %>% 
  filter(Site %in% check_sites) %>% 
  filter(Block %in% check_blocks) %>% 
  filter(Treatment %in% check_trtmts) %>% 
  group_by(Site, Block, Treatment, Date, Wavelength) %>% 
  summarize(
    avg_reflect = mean(Reflectance),
    max_ref = max(Reflectance),
    min_ref = min(Reflectance),
    sd_reflect = sd(Reflectance)
    ) 

ggplot(data = df_block, mapping = aes(x = Wavelength, y = avg_reflect)) +
  geom_line(aes(color=Treatment)) + 
  geom_ribbon(aes(ymin=avg_reflect-sd_reflect, ymax=avg_reflect+sd_reflect, fill=Treatment), alpha=0.25) +
  facet_grid(Date ~ Site + Block ) + 
  scale_color_manual(values=c("CT" = "forestgreen", "CT1"="darkolivegreen2", "CT2"="darkolivegreen3",
                              "N" = "dodgerblue", "NO3" = "skyblue", "NH4" = "deepskyblue",
                              "P" = "red2",
                              "NP" = pur_pal[5],
                              "F0.5" = pur_pal[1],
                              "F1" = pur_pal[2],
                              "F2" = pur_pal[3],
                              "F5" = pur_pal[4],
                              "F10" = pur_pal[5])) + 
  scale_fill_manual(values=c("CT" = "forestgreen", "CT1"="darkolivegreen2", "CT2"="darkolivegreen3",
                              "N" = "dodgerblue", "NO3" = "skyblue", "NH4" = "deepskyblue",
                              "P" = "red2",
                              "NP" = pur_pal[5],
                              "F0.5" = pur_pal[1],
                              "F1" = pur_pal[2],
                              "F2" = pur_pal[3],
                              "F5" = pur_pal[4],
                              "F10" = pur_pal[5]))

```


 
## Plot Spectra: Site vs. Date
Average plot averages by block. NOTE: Ask Laura what the correct error propogation is here.

```{r plot_spectra_dates, echo=F}
#PLOT SUBSET -- SPECIFY SITE/DATE/ETC to ZOOM IN ON
check_dates <- ymd("2017-07-21", "2017-06-15")
check_sites <- c("MAT", "LMAT")
check_blocks <- c("B1", "B2", "B3", "B4")
check_trtmts <- c("NP", CT) 

# Data Comparison Format
df_dates <- df_tidy %>% 
  filter(Date %in% check_dates) %>% 
  filter(Site %in% check_sites) %>% 
  filter(Block %in% check_blocks) %>% 
  filter(Treatment %in% check_trtmts) %>%
  group_by(Site, Block, Treatment, Date, Wavelength) %>% # average measurements by plot
  summarize(
    avg_reflect = mean(Reflectance),
    max_ref = max(Reflectance),
    min_ref = min(Reflectance),
    sd_reflect = sd(Reflectance)
    )  %>% 
  group_by(Site, Treatment, Date, Wavelength) %>% # average plots by block
  summarize(
    block_avg_reflect = mean(avg_reflect),
    max_ref = max(avg_reflect),
    min_ref = min(avg_reflect),
    sd_reflect = sd(avg_reflect)
    ) 

ggplot(data = df_dates, mapping = aes(x = Wavelength, y = block_avg_reflect)) +
  geom_line(aes(color=Treatment)) + 
  geom_ribbon(aes(ymin=block_avg_reflect-sd_reflect, ymax=block_avg_reflect+sd_reflect, fill=Treatment), alpha=0.25) +
  facet_grid(Site ~ Date) + 
  scale_color_manual(values=c("CT" = "forestgreen", "CT1"="darkolivegreen2", "CT2"="darkolivegreen3",
                              "N" = "dodgerblue", "NO3" = "skyblue", "NH4" = "deepskyblue",
                              "P" = "red2",
                              "NP" = pur_pal[5],
                              "F0.5" = pur_pal[1],
                              "F1" = pur_pal[2],
                              "F2" = pur_pal[3],
                              "F5" = pur_pal[4],
                              "F10" = pur_pal[5])) + 
  scale_fill_manual(values=c("CT" = "forestgreen", "CT1"="darkolivegreen2", "CT2"="darkolivegreen3",
                              "N" = "dodgerblue", "NO3" = "skyblue", "NH4" = "deepskyblue",
                              "P" = "red2",
                              "NP" = pur_pal[5],
                              "F0.5" = pur_pal[1],
                              "F1" = pur_pal[2],
                              "F2" = pur_pal[3],
                              "F5" = pur_pal[4],
                              "F10" = pur_pal[5]))

```


## Save Corrected Spectral Data

```{r save_spectra, echo=T}
save <- F

if (save == T) {
  # SAVE CORRECTED SPECTRA if you want the dataframe or .csv for later
  unispec_data <- df_tidy %>%
    filter(Type=="correct") %>%
    select(-Type)
  write_rds(unispec_data, "unispec_data_correct.rds")
} 

```


# Check Vegetation Indices

Currently, this only works for NDVI, EVI, and EVI2 as I haven't worked out spectral interpolation yet and the other indices need reflectance at a specific value (not a range). 

## Plot NDVI: Site vs. Block
```{r ndvi, echo=F}
#PLOT SUBSET -- SPECIFY SITE/DATE/ETC to ZOOM IN ON
check_dates <- df_table %>% select(Date) %>% pull()#ymd("2017-06-08", "2017-06-15", "2017-06-26")
check_sites <- c("MAT", "LMAT")
check_blocks <- c("B1", "B2", "B3", "B4")
check_trtmts <- c("NP", CT) 

ndvi_types <- df_tidy %>% 
  filter(Date %in% check_dates) %>% 
  filter(Site %in% check_sites) %>% 
  filter(Block %in% check_blocks) %>% 
  filter(Treatment %in% check_trtmts) %>%
  calculate_index(indices = "NDVI") # function in "unispec_functions.R"

ndvi_plot <- ndvi_types %>% 
  group_by(Site, Block, Treatment, Date) %>% 
  summarise(
    avg_ndvi = mean(NDVI),
    sd_ndvi = sd(NDVI)
  ) %>% 
  group_by(Site,Treatment,Date) %>% 
  summarise(
    avg_ndvi = mean(avg_ndvi),
    sd_ndvi = sd(sd_ndvi)
  )

ggplot(data = ndvi_plot, mapping = aes(x = Date, y = avg_ndvi, color=Treatment)) +
  geom_point() +
  geom_line() + 
  geom_errorbar(aes(ymin = avg_ndvi-sd_ndvi, ymax= avg_ndvi + sd_ndvi), width=2) + 
  facet_wrap(vars(Site)) + 
  scale_color_manual(values=c("CT" = "forestgreen", "CT1"="darkolivegreen2", "CT2"="darkolivegreen3",
                              "N" = "dodgerblue", "NO3" = "skyblue", "NH4" = "deepskyblue",
                              "P" = "red2",
                              "NP" = pur_pal[5],
                              "F0.5" = pur_pal[1],
                              "F1" = pur_pal[2],
                              "F2" = pur_pal[3],
                              "F5" = pur_pal[4],
                              "F10" = pur_pal[5])) 
```

## Save Vegetation Index Data
Append data to yearly r dataframe or .csv file. Write code to read in saved data and remove duplicate rows. >> ASK JIM: I'm not sure what the best data organization system is yet. 


