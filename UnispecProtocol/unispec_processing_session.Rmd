---
title: "Process Unispec Files"
author: "Ruby An"
date: "6/17/2019"
output: html_document
editor_options: 
  
  chunk_output_type: console
params:
  
  
  function_file: unispec_protocol_functions.R
  session_date: 2020-07-19
  data_path: !r getwd () # select folder where spu files reside
---

## Required Packages
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, message=F)

## Required Packages
library("tidyverse")
library("knitr")
library(rChoiceDialogs)
library(lubridate)

## Functions file  ## Decide to include functions in processing session for now. 
```

## R Functions
```{r functions, include = F}
read_spu_file_metadata <- function(filename) {
  # DESCRIPTION: Reads first 9 text lines in .spu files 
  # INPUT: .spu file -- collected using PPSystems UnispecDC
  # OUTPUT: dataframe with 15 columns on instrument status, scan settings, max/min value

    # Extract info from the file itself, reading metadata from first 9 lines. Create a dataframe
    text <- read_lines(filename, n_max=9)
    
    # Line 1: Extract the file name in the spu file as a check. Some file names have spaces 
    spu_filename <- str_replace(text[1],".*[\\\\]([A-z0-9.]\\s*)","\\1") %>% # extract filename
      str_replace("\"","") # removes trailing quote at end of line 
    FileNum <- str_extract(spu_filename, "\\d{5}") %>% as.numeric() # from 5 digits in filename
    
    # Line 2: 
    Remarks <- str_split(text[2], pattern = " ")[[1]] # split by "space"
    Type <- str_split(Remarks[7], pattern = "=")[[1]][2] # extract relevant part
    ScanType <- ifelse(grepl("DARKscan",Type, fixed=T), "DARKscan", # format
                       ifelse(grepl("Datascan,DC",Type, fixed=T), "Throwawayscan", 
                              Type))
    DarkscanID <- str_extract(text[2], "Dark=.+.spu")
    Remarks <- text[2]
    
    # Line 3: 
    DateTime <-  lubridate::mdy_hms(text[3], tz="America/Anchorage")
    
    # Line 4: Limits -- range of spectra measured
    Limits <- str_extract(text[4], "\\d+.\\d.+\\d") 
    
    # Line 5: 
    Temperature <- as.numeric(strsplit(strsplit(text[5], split = " ")[[1]][4], split="=")[[1]][2])
    Battery <- str_extract(text[5], "BattV=\\d+.\\d+")
    Aux <- str_extract(text[5], "A\\d=.+\\d")
    
    # Line 6-9: 
    Minimum <- str_extract(text[6], "\\d+.+\\d") # Wavelength, ChB min
    Minimum_wavelength <- str_split(Minimum, boundary("word"))[[1]][1] # Wavelength
    Minimum_value <- str_split(Minimum, boundary("word"))[[1]][2] # ChB AD
    
    Maximum <- str_extract(text[7], "\\d+.+\\d") # Wavelength, ChB max
    Maximum_wavelength <- str_split(Maximum, boundary("word"))[[1]][1]
    Maximum_value <- str_split(Maximum, boundary("word"))[[1]][2]

    Integration <- as.numeric(strsplit(text[8], split = " ")[[1]][3])
    NumberScans <- str_extract(text[9], "\\d+")
    
    # Truncated Filename - use as SCANID to join to other dataframes
    spu_filename <- unlist(str_split(filename, pattern = "/")) %>% last()
    
    # Metadata 
    metadata <- tibble(spu_filename, DateTime, FileNum, ScanType, Integration, NumberScans,Minimum_wavelength,Minimum_value, Maximum_wavelength, Maximum_value, Limits, Temperature, Battery, Aux, DarkscanID, Remarks)

  # Print filenames while reading 
  #print(spu_filename) # use for error checking
  
  return(metadata)
}


read_spu_file_spectra <- function(filename) {
  # DESCRIPTION: For a generic .spu file regardless of name, extract spectral data
  # INPUT: Unispec-DC .spu file
  # OUTPUT: dataframe of spectral data with 3 columns Wavelength, ChB, ChA
  
  # Read spectral intensity data into dataframe
  data <- read.table(file = filename, skip = 9, col.names = c("Wavelength", "ChB", "ChA"))
  
  return(data)
}

```



## Data directories, date and key files

  * Session_date in yyyy-mm-dd form.
  
  * data_path is year folder where the session files are stored. The structure is year/spu_orig/yyyy-mm-dd  where yyyy-mm-dd is sample date. Should be on OneDrive.
  
  * Field Key file with header "Date,Site,Block,Treatment,Replicate,Location,FileNum,Notes,Weather".  File name convention is year_unispec_field_key.csv
  
```{r, include = F}
## Session Date

session_year <- year(params$session_date)

# ## use the params$data_path as starting folder
# data_path <- params$data_path
# 
# # Check if directory exist and use it as the working directory
# if (!dir.exists(data_path) || !grepl(session_year,data_path)) {
#   data_path <- rchoose.dir(caption = "Select Unispec Data directory")}
# setwd(data_path)
# 
# ## Field key file
# field_key <- file.path(data_path,paste0(session_year,"_unispec_coordinate_key.csv"))
# 
# ## Check if file exist
# if (!file.exists(field_key)) {field_key <- rchoose.files(caption = "Select field key file")}

data_path <- "C:/Users/toolik/Desktop/Toolik2020/UnispecData/2020-07-19"


```

## Digitize Data Locally 


### Read .SPU files 
  
```{r, include = F}
## Make a character vector of all .spu raw files in current session date folder
spu_folder <- file.path(data_path)
spu_files <- list.files(path = spu_folder, pattern = ".spu$", full.names = T, recursive=T)

# Read metadata text lines (9) from the spu files
spu_filedata <- map_dfr(spu_files, read_spu_file_metadata)

# Add Metadata columns:
#   spu_filename_full: variable with the full path filename 
#   Site: column from spu_filename
#   FileNum: column from spu_filename 
#   ScanType: column identifying scan type (Dark, Throwaway, Datascan)
spu_metadata <- spu_filedata %>% 
  mutate(spu_filename_full = spu_files) %>% #
  mutate(Site = toupper(str_extract(spu_filename, "[A-Za-z]{3}[0-9]{1,2}(?=_)"))) %>% # get string that is 3 letters, 2 numbers before _ 
  mutate(FileNum = as.integer(str_extract(spu_filename, "\\d{5}"))) 

# Read spectra from .spu files > add to metadata
spu_dataframe <- spu_metadata %>% 
  mutate(Spectra=map(spu_filename_full, function(x) read_spu_file_spectra(x))) %>% 
  mutate(Date = date(DateTime))
  # mutate(Time = DateTime-floor_date(DateTime, unit = "day")) 

```


### Read Field Key 
```{r, echo = F, warning=F}

# Find files 
key_files <- list.files(path=data_path, pattern="*key.csv", full.names=TRUE)

field_keys <- key_files %>%  purrr::map(function(file_name) read_csv(file_name)) %>% 
  reduce(rbind) %>% 
  mutate(Date = date(params$session_date)) # Add date

# Confirm spu_dataframe has same Site abbreviation as key
if(!(unique(spu_dataframe$Site) %in% unique(field_keys$Site) %>% all())) {
  print("SITE NAME in spu_files and spu_fieldkey differ! Check file names and file key template.")
}

# Date inconsistencies
if(spu_dataframe %>% pull(Date) %>% unique() != params$session_date){
  print("Date Discrepancy")
}

# Join by SITE, DATE, FILENUM
df <- left_join(spu_dataframe, field_keys) %>% arrange(DateTime) %>% 
  mutate_at(.vars = vars(Site, Block, Treatment), .funs = factor)
```


## Data Summary 
```{r data_summary} 
## Data summary
spu_dataframe %>%
  mutate(Date = floor_date(DateTime, unit="day")) %>% 
  group_by(Date, Site) %>% 
  summarize(Files = n_distinct(spu_filename)) %>% 
  kable(caption = "Raw .spu files read-into spu_dataframe")

## Print Summary of files 
df %>% group_by(Date, Site, Block) %>%  
  filter(!is.na(Treatment)) %>% 
  summarize(Treatments = str_c(str_replace_na(sort(unique(Treatment))), collapse = ","), Num_Files = n()) %>% 
  kable(caption ="Files listed in field key")

```


## QAQC

### Missing files 
List missing raw .spu files and those that are unprocessed (raw .spu file does not appear in summary .xlsx). Missing raw files. 
```{r missing, warnings=F}
### MISSING: Find all processed data that is missing corresponding raw spu files 
missing_spu_data <- anti_join(field_keys, spu_dataframe)

if(nrow(missing_spu_data) > 0){
  missing_spu_data 
}

(unprocessed_files <- anti_join(spu_dataframe, field_keys) %>% pull(spu_filename))
```


### Mis-Labeling 
Check for human error in recording correspondence between metadata and file numbers. 

#### Discrepancy Checks
Check for possible mislabeling. 

```{r df_check, dependson="df", eval= T, echo=F, warnings=F}

## General NA test
df_na <- df %>% filter(ScanType == "Datascan") %>% 
  filter(is.na(spu_filename) | # spu_filename
           is.na(Site) | # site
           is.na(Block) & !str_detect(Treatment, "REF|DARK|VEG") | # Block NA's should always be REFS or EXTRA
           is.na(Replicate) & !str_detect(Treatment, "REF"), # Check for replicate NA's that aren't REF
         Treatment != "EXTRA|VEG|REF") # don't care about EXTRA 


## Check for large filenumbers : Inconsistent File Number reading due to number at end of site names
df_lgfn <- df %>%
  filter(FileNum > 600) %>%  # Sites w/numbers at the end mis-read in as part of file number. All > 100000: Unispec-DC measurements only go up to 5 digits.
  select(Date, Site, Block, Treatment, FileNum, spu_filename)


## Check File Number Pattern -- pulls out treatments that are not multiples of 5
#### Num Files per block should be multiples of 5, unless REF or NA
df_filenum_count <- field_keys %>% group_by(Site, Date, Block) %>% 
  filter(Treatment != "THROWAWAY") %>% 
  filter(Treatment != "DARK") %>% 
  filter(Treatment != "REF") %>% 
  summarize(Treatments = str_c(str_replace_na(unique(Treatment)), collapse = ","), Num_Files = n()) %>% 
  filter(Num_Files %% 5 != 0)  # files per plot

```


Missing labels: 
`r if(nrow(df_na)>0){kable(df_na)}`
Large filenumbers: 
`r if(nrow(df_lgfn)>0){kable(df_lgfn)}`

An abnormal numbers of files per plot: 
`r if(nrow(df_filenum_count)>0){kable(df_filenum_count)}`

#### Duplicates 
Identify duplicates in field key or spu_files. Fix by editing unispec_key_fix.csv or deleting duplicate files. Only duplicates should be REF files used for other sites. 

```{r duplicates}

### Check for duplicate entries in key
duplicate_entries <- field_keys %>% group_by(Date, Site, Block, Treatment, Replicate, FileNum) %>%
  filter(n()>1) %>%
  arrange(Date, Site, FileNum) 

if(nrow(duplicate_entries)>0) {
  duplicate_entries %>% kable()
}

### Check raw spu_files have no duplicates

duplicates <- spu_dataframe %>% group_by(DateTime, FileNum) %>%
  filter(n()>1) %>%
  arrange(Date, Site, FileNum)

## File List 
duplicate_files <- duplicates$spu_filename
  
if (nrow(spu_dataframe) != length(unique(spu_dataframe$DateTime))) {
    ## Check for duplicate spu files 
  duplicates %>%   kable()
}


```


### Instrument Check:

#### Max'd Out Spectra (> 65000 AD)
List files that maxed out, in the wavelengths used to calculate MODIS NDVI.
```{r maxed, dependson="df"}

maxed_data <- df %>% inner_join(spu_dataframe %>% select(spu_filename)) %>% # restrict to those w/.spu files
  unnest(Spectra) %>% 
  filter(ChA > 65000 | ChB > 65000) %>% 
  group_by(spu_filename) %>% 
  summarize(maxed_number = n(), maxed_wavelengths = str_c(min(Wavelength), " - ", max(Wavelength), collapse = ", "))  

# Print MAXED files 
if (nrow(maxed_data) > 0) {
  df %>% select(spu_filename, Site, Date, Block, Treatment, Replicate) %>% 
     inner_join(maxed_data)
  
} else { print("no maxed files")}

# Maxed file list 
maxed_files <- maxed_data$spu_filename
```


#### Dim'd Out Spectra
Primarily Darkscans should show up. 
```{r dim, dependson=df, warnings=F}
dim_data <- df %>% inner_join(spu_dataframe %>% select(spu_filename)) %>% 
  #filter(!ScanType %in% c("Throwawayscan", "DARKscan")) %>% 
  unnest(Spectra) %>% 
  group_by(spu_filename) %>% 
  summarize(ChA_max = max(ChA)) %>% 
  filter(ChA_max < 20000) %>% 
  left_join(df) 

# Print DIM files  
if (nrow(dim_data) > 0) {
  dim_data %>% select(spu_filename, Site, Date, Block, Treatment, Replicate, ChA_max) %>% 
    kable()
  
} else { print("no dim files")}

# File List
dim_files <- dim_data$spu_filename
```


#### Zero'd Spectra
```{r zerod}
## zero'd data 
zero_data <- df %>% inner_join(spu_dataframe %>% select(spu_filename)) %>% unnest(Spectra) %>% 
  filter(ChA == 0) %>%  ## This is for all wavelengths, not just 400-1000nm
  group_by(spu_filename) %>% 
  summarize(Zeros = n()) %>% 
  left_join(df)

## File List : Restricted Wavelengths
zero_files <- zero_data %>% unnest(Spectra) %>% 
  filter(ChA == 0 ) %>% 
  filter(Wavelength > 400, Wavelength < 1000) %>% 
  pull(spu_filename) %>% unique()

zero_data_narrowed <- df %>% filter(spu_filename %in% zero_files) 

# Print MAXED files 
if (nrow(zero_data_narrowed) > 0) {
  zero_data_narrowed %>% select(spu_filename, Site, Date, Block, Treatment, Replicate, Zeros)
  
} else { print("no zero'd files")}
  
```


#### Time Check / Editable Data Table
Times between files, check for Mislabeling, etc.
```{r time_check}
# Select columns to Check
timedata <- df %>% 
  filter(Site == "DHT89") %>% 
  select(Site, Date, DateTime, FileNum, Integration, ScanType) %>% 
  distinct(DateTime, .keep_all = T)

timedata$diff <- timedata$DateTime - lag(timedata$DateTime)

meta_timedata <- left_join(timedata, field_keys)

time_check <- meta_timedata %>% select(Site, Date, DateTime, Block, Treatment, Replicate, Location, FileNum, diff, everything()) %>% ungroup()

# Examine dataframe
time_check %>% select(Date, Site, Block, Treatment, Location,FileNum, diff, Integration) %>% print(n=200)
```


### Raw Spectra Plot Check
```{r check_refs}


df %>% 
  
  # select subset
  filter(Site == "MNT97") %>% 
  # arrange for 
  unnest(Spectra) %>% 
  filter(Wavelength > 400, Wavelength < 1000) %>% 
  mutate(Reflectance = ChB/ChA) %>% 
  gather(key = Channel, value = Intensity, ChB, ChA) %>% 
  gather(key = ref_part, value = Reflectance_Intensity, Intensity, Reflectance) %>% 
  
  # Viz
  ggplot(mapping = aes(x = Wavelength, y = Reflectance_Intensity, group = FileNum)) +
  geom_line(aes(color = Treatment)) +
  facet_grid(ref_part ~ FileNum, scales="free") 


```



## REF Correction

### Confirm REF choice

White references correct for instrument & cable irregularities. Multiplying by the correction factor (ChA/ChB) smooths out the spectra. There are typically 5 reference measurements per *Date* / *Site*. If multiple file numbers are listed, the correction factors are averaged. 

Based on the original field notebook key and the following quality checks, choose reference files by entering the appropriate file numbers in **`r field_key`** for the rows where the column *Treatment* = **REF**. 

Make sure to rerun the **Load Keys**. The following plots your chosen references.

### Extract REF Data
```{r ref_table, echo=F}
options(knitr.kable.NA = '')

## Build REF key 
ref_keys <- field_keys %>% 
  filter(Treatment == "REF") %>% # extract reference data 
  ## The following separates the Site column into "Site" and "Site_REF"
  ### Site = the site to which the reference measurements should be applied 
  ### Site_REF = the site where the reference measurements were actually collected
  separate(Site, into=c("Site", "Site_REF"), sep = "_", fill="right") %>% 
  mutate(Site_REF = coalesce(Site_REF, Site)) # if the references were collected at 'Site', the created column Site_REF will be NA. Use coalesce() to fill these NA's with the value of "Site".  

### spu data for references
spu_for_ref <- spu_dataframe %>% # in "spu_dataframe", the "Site" column is the location where the data was collected 
  rename(Site_REF = Site) # we thus rename Site to Site_REF to match the column 'ref_keys'

## Join spu data to ref_keys by Site_REF, Date, FileNum
ref_data <- ref_keys  %>% 
  left_join(spu_for_ref)  ## "Site_REF" is the location where the file (from which the reference correction factor is calculated) actually was collected
  ## "Site", inherited from ref_keys, is now the location where the correction factor should be applied


## Table per Site of Reference Files 
ref_filenames_table <- ref_data %>% group_by(Date, Site, Site_REF) %>% 
  summarize(Files = n_distinct(spu_filename), ref_filenames = str_c(spu_filename,collapse = ", ")) 

## Table per Site for all files 
df_ref_table <- df %>% 
  separate(Site, into=c("Site", "Site_REF"), sep = "_", fill="right") %>%
  filter(!is.na(Treatment)) %>% 
  group_by(Date, Site) %>% 
  summarize(Treatments = str_c(unique(Treatment), collapse = ","), Files = n_distinct(spu_filename))

## Output
ref_filenames_table
```


### Plot References
```{r ref_choice_list}

ref_data %>% filter(Spectra %>% map(is.null) %>% map_lgl(any)) %>% select(Date, Site, Block) %>% unique()

## Build Plot all reference data files
ref_data_all <- ref_data %>% unnest(Spectra) %>%
  filter(Wavelength > 400, Wavelength < 1000) %>% 
  mutate(CorrectionFactor = ChA/ChB)

(ref_plot_all <- ggplot(ref_data_all, aes(x = Wavelength, y = CorrectionFactor, group=spu_filename)) + 
  theme(legend.position="left") + 
  geom_line(aes(color=factor(Integration))))


## Build Plot all reference mistakes
ref_data_mistakes <- ref_data_all %>%
  filter(CorrectionFactor > 5) %>% 
  distinct(spu_filename) %>% 
  select(spu_filename) %>% 
  left_join(ref_data_all)

(ref_plot_mistakes <- ggplot(ref_data_mistakes, aes(x = Wavelength, y = CorrectionFactor)) +
  geom_line(aes(color=spu_filename)))

## PLOTS
ref_plot_all + ggtitle("ALL REFERENCES")
# ref_plot_mistakes + ggtitle("REF Mistakes") +
#   scale_y_continuous(limits = c(0, NA))

## File Lists
ref_files <- ref_data$spu_filename %>% unique()
ref_mistakes <- ref_data_mistakes %>% distinct(spu_filename) %>% pull()


```

### Apply References
Rerun the **Join Data & keys** sections above to update the `df_clean` dataframe.Apply your chosen references to actual spectral data to create the tidy dataframe `df_tidy` containing corrected sepectral reflectance values.

```{r apply_refs, echo=FALSE}
## Average 5 chosen ref measurements per DATE/SITE/BLOCK 
ref_summary <- ref_data %>% 
  ## The following steps expand the "Block" column to create one REF set per Block per Site. This structure is necessary for situtations where different refs are used for different blocks at the same site. 
  separate(Block, into = c("BX1", "BX2", "BX3", "BX4"), sep = ",") %>% #1: expand string entry in "Block" into separate columns -- NOTE: this step throws a "Warning: Expected 4 pieces." for sites w/less than 4 blocks
  gather(Block, BlockString, BX1:BX4) %>% #2: re-condense into one column, generates correct number of rows per site AND per block
  mutate(Block = str_squish(BlockString), BlockString=NULL) %>% #3: replace placeholder column names w/"B1-B4". Also removes whitespace from BlockString contents introduced by "separate" function
  filter(!is.na(Block)) %>% #4: remove empty rows for sites w/out B3 or B4
  ### Unnest Spectra & calculate
  unnest(Spectra) %>% 
  filter(Wavelength > 400, Wavelength < 1000) %>% 
  mutate(CorrectionFactor = ChA/ChB) %>% 

  ### The following code group repeated REF measurements, and takes the mean 
  group_by(Date,Site,Block,Integration, Wavelength) %>% 
  # group_by(Date,Site,Block,Wavelength, Integration_ms) %>% # to separate integration times
  summarize(ChA_REF = mean(ChA), ChB_REF = mean(ChB), CorrectionFactor = mean(ChA/ChB), Notes_REF = str_c(Notes, collapse = "; "), ref_filenames = str_c(spu_filename,collapse = ", "))

## Join DATA with REFS
### Check no missing Spectra
spu_dataframe %>% filter(Spectra %>% map(is.null) %>% map_lgl(any))

spu_for_plots <- df %>% filter(!str_detect(Treatment, "REF|DARK")) %>% filter(!is.na(DateTime)) %>% 
  unnest(Spectra) %>% 
  filter(Wavelength > 400, Wavelength < 1000)


df_ref <- left_join(spu_for_plots, ref_summary) %>%
  mutate(raw_reflectance = ChB/ChA) %>% # the raw reflectance
  mutate(corrected_reflectance = raw_reflectance*CorrectionFactor) 


## Corrected Reflectances 
df_corrected <- df_ref %>% 
  nest(processed_spectra = c(Wavelength, ChB, ChA, ChB_REF, ChA_REF, CorrectionFactor, raw_reflectance, corrected_reflectance))
```

##### Check Application
```{r}
## Check all files have a corrected reflectance
corrected_spectra_files <- df_corrected %>% unnest(processed_spectra) %>% filter(!is.na(corrected_reflectance)) %>% select(spu_filename, Date, Site) %>% distinct()

## missing corrections
anti_join(df_corrected, corrected_spectra_files) %>% 
  group_by(Date, Site, Block, Treatment, Integration) %>% 
  summarize(Num_Files = n()) %>% kable()

## Check that no important treatments are left out 
anti_join(df, df_corrected, by = "spu_filename") %>% pull(Treatment) %>% unique()
```

#### Reflectance >1 
```{r absurd}

## Corrected Spectra
absurd_data <- df_corrected %>% 
  filter(!str_detect(Treatment, "REF")) %>% # ignore REF files
  unnest(processed_spectra) %>% # use corrected spectra
  select(-(ChB:raw_reflectance)) %>%  # remove unnecessary Wavelength specific rows 
  filter(Wavelength > 400 & Wavelength < 1000) %>% 
  filter(corrected_reflectance > 1) %>% 
  nest(Spectra = c(Wavelength, corrected_reflectance))

## File List 
absurd_files <- absurd_data$spu_filename

```


##### Plot Check
```{r}

(df_corrected %>% 
  slice(1:10) %>% 
  unnest(processed_spectra) %>% 
  gather(key = Status, value = Reflectance, raw_reflectance, corrected_reflectance) %>% 
  
  # VIZ
  ggplot(mapping = aes(x = Wavelength, y = Reflectance )) + 
  geom_line(aes(color = spu_filename, linetype = Status)) + 
    facet_wrap(vars(Date, Site, Block, Treatment, FileNum)))

```


## ? INTERPOLATION


## SAVE DATA
```{r}
processed_spu_data <- df_corrected %>%
  select(spu_filename, DateTime, ref_filenames, processed_spectra) %>% unnest(processed_spectra) %>%
  select(spu_filename, DateTime, ref_filenames, Wavelength, ChB, ChA, raw_reflectance, CorrectionFactor, corrected_reflectance) %>%
  nest(Spectra = c(Wavelength, ChB, ChA, raw_reflectance, CorrectionFactor, corrected_reflectance))

ref_spu_data <- ref_data %>% unnest(Spectra) %>%
  select(spu_filename, DateTime, Wavelength, ChB, ChA, Reflectance) %>%
  mutate(CorrectionFactor = 1/Reflectance, corrected_reflectance = NA) %>%
  filter(Wavelength > 400, Wavelength < 1000) %>%
  rename(raw_reflectance = Reflectance) %>%
  nest(Spectra = c(Wavelength, ChB, ChA, raw_reflectance, CorrectionFactor, corrected_reflectance))

df_processed <- bind_rows(processed_spu_data, ref_spu_data)

## Save
# processed_spu_filename <- paste0(data_path, dir_year, "_processed_spu_data.rds")
#
# write_rds(df_processed, processed_spu_filename)
```




## Calculate INDICES

Currently, this only works for NDVI, EVI, and EVI2 as I haven't worked out spectral interpolation yet and the other indices need reflectance at a specific value (not a range).

### Calculate Indices
```{r}

df_processed %>%  filter( Spectra %>% map(is.null) %>% map_lgl(any)) # remove files w/out spu_spectra

df_indices <- df %>% select(-Spectra) %>%
  filter(!is.na(Treatment)) %>%
  filter(!str_detect(Treatment, "REF|DARK")) %>%
  inner_join(df_processed) %>%
  ## Format for calculuating indices
  unnest(Spectra) %>%
  select(-ChB, -ChA, -raw_reflectance, -CorrectionFactor) %>%
  rename(Reflectance = corrected_reflectance) %>%
  nest(Spectra = c(Wavelength, Reflectance)) %>%
  ## Calculate NDVI
  mutate(Indices = map(Spectra, function(x) calculate_indices(x, band_defns = band_defns, instrument = "MODIS", indices = c("NDVI", "EVI", "EVI2"))))

index_filename <- paste0(data_path, params$session_date, "_index_data.rds")
write_rds(df_indices, index_filename)


## convert to SHINY ready format


```


### Calculate INDICES for new spu files
```{r}
new_files <- spu_dataframe_new$spu_filename

df_processed_new <- df_processed %>% filter(spu_filename %in% new_files)

indices_new <- unispec_file_key_fix %>%
  filter(!is.na(Treatment)) %>%
  filter(!str_detect(Treatment, "REF|DARK")) %>%
  inner_join(df_processed_new) %>%
  ## Format for calculuating indices
  unnest(Spectra) %>%
  select(-ChB, -ChA, -raw_reflectance, -CorrectionFactor) %>%
  rename(Reflectance = corrected_reflectance) %>%
  nest(Wavelength, Reflectance,.key = Spectra) %>%
  ## Calculate NDVI
  mutate(Indices = map(Spectra, function(x) calculate_indices(x, band_defns = band_defns, instrument = "MODIS", indices = c("NDVI", "EVI", "EVI2"))))

## SAVE UPDATED INDICES
indices_all <- bind_rows(df_indices, indices_new)
index_filename <- paste0(data_path, dir_year, "_index_data.rds")
write_rds(indices_all, index_filename)

```


### Visualize with other years

### Check across years


## CHECKLIST/SUMMARY REPORT

### Label Unispec Problems
```{r unispec_problem_key}

problem_files <- c(maxed_files, dim_files, absurd_files, missing_spu_files)

df_ref_filenames <- df_corrected %>% unnest(processed_spectra) %>%
  select(spu_filename, ref_filenames) %>% distinct()

unispec_problem_key_pre <- unispec_file_key_fix%>%
  mutate(Replicate = factor(Replicate)) %>%
  # mislabeled
  mutate(mislabeled = !is.na(key_fix)) %>% # mislabeled
  # mismeasurement
  left_join(maxed_data) %>% # maxed_number, maxed_wavelenghs
  mutate(maxed = spu_filename %in% maxed_files) %>%
  mutate(dim = spu_filename %in% dim_files) %>% # dim
  mutate(absurd_reflectance = spu_filename %in% c(zero_files, absurd_files)) %>% # absurd_reflectance
  # missing
  mutate(missing_spu = spu_filename %in% missing_spu_files) %>%  # missing_spu
  left_join(df_ref_filenames)

# miscorrection
ref_problem_key <- unispec_problem_key_pre %>%
  filter(spu_filename %in% ref_files) %>% # select ref files
  #mutate(mislabeled = if_else(spu_filename %in% ref_mistakes, TRUE, mislabeled)) %>%
  gather("problem", "status", mislabeled, maxed, dim, absurd_reflectance, missing_spu) %>% # row for each type of problem
  filter(!is.na(status)) %>% # remove non-problems
  filter(status != FALSE) %>% # remove non-problems
  group_by(spu_filename) %>%
  select(spu_filename, problem, status) %>%
  summarize(problems = str_c(unique(problem), collapse = ", ")) %>% # group: one row per spu_filename
  rename(ref_filenames = spu_filename, ref_problem = problems) # rename variables to join w/unispec_problem_key

## Unispec Problem Key
unispec_problem_key <- unispec_problem_key_pre %>%
  separate_rows(ref_filenames, sep = ", ") %>% # split ref_filenames string into multiple rows per ref_filename
  left_join(ref_problem_key) %>% # adds ref_problem column
  group_by(spu_filename) %>%
  summarize(ref_problems = str_c(str_replace_na(ref_problem), collapse = "; ")) %>% # collapse to one row per spu_filename
  right_join(unispec_problem_key_pre) %>%  # add the rest of the key info back in
  mutate_at(.vars = c("ref_problems", "dim", "absurd_reflectance", "mislabeled", "missing_spu"), .funs = factor) %>%
  mutate(file_problem = spu_filename %in% problem_files) %>%
  select(spu_filename, Type, Date, Site, Block, Treatment, Replicate, Weather, Notes, missing_spu, file_problem, mislabeled, key_fix, maxed, maxed_number, maxed_wavelengths, dim, absurd_reflectance, ref_problems, ref_filenames) # order


### Summary table
unispec_problem_key %>%
  mutate_at(list(factor), .vars = vars(c("Site", "Block", "Treatment", "key_fix", "ref_problems"))) %>%
  mutate(file_problem = spu_filename %in% problem_files) %>%
  summary()
```


